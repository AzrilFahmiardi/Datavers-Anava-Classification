{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14452162,"sourceType":"datasetVersion","datasetId":9230974}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2b4cb422","cell_type":"markdown","source":"## 1. Configuration & Setup","metadata":{}},{"id":"0ba4bde3","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom datetime import datetime\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nimport gc\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\n\nprint(\"Libraries imported successfully.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:19:36.317735Z","iopub.execute_input":"2026-01-11T14:19:36.318303Z","iopub.status.idle":"2026-01-11T14:19:36.324142Z","shell.execute_reply.started":"2026-01-11T14:19:36.318276Z","shell.execute_reply":"2026-01-11T14:19:36.323388Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Libraries imported successfully.\n","output_type":"stream"}],"execution_count":16},{"id":"9d95450b","cell_type":"code","source":"USE_GPU = True  \n\ngpu_config = {\n    'xgboost_gpu': USE_GPU\n}","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:19:38.178901Z","iopub.execute_input":"2026-01-11T14:19:38.179205Z","iopub.status.idle":"2026-01-11T14:19:38.182887Z","shell.execute_reply.started":"2026-01-11T14:19:38.179181Z","shell.execute_reply":"2026-01-11T14:19:38.182306Z"},"trusted":true},"outputs":[],"execution_count":17},{"id":"41cd27c8","cell_type":"code","source":"class Config:\n    DATA_PATH = '/kaggle/input/ride-hailing-trip-classification-dataset/'\n    \n    N_FOLDS = 5\n    RANDOM_STATE = 42\n    TARGET_COL = 'Trip_Label'\n    ID_COL = 'Trip_ID'\n    \n    USE_TEMPORAL = True\n    USE_DISTANCE = True\n    USE_SENSOR_AGG = True\n    USE_ECONOMIC = False\n    USE_INTERACTION = True\n    \n    @staticmethod\n    def get_catboost_params(use_gpu=False):\n        params = {\n            'iterations': 1000,\n            'learning_rate': 0.05,\n            'depth': 6,\n            'loss_function': 'MultiClass',\n            'eval_metric': 'TotalF1:average=Macro',\n            'auto_class_weights': 'Balanced',\n            'random_seed': 42,\n            'verbose': 100,\n            'early_stopping_rounds': 50\n        }\n        if use_gpu:\n            params['task_type'] = 'GPU'\n            params['devices'] = '0'\n            print(\"  CatBoost: GPU mode activated\")\n        else:\n            params['task_type'] = 'CPU'\n            print(\"  CatBoost: CPU mode\")\n        return params\n    \n    @staticmethod\n    def get_lightgbm_params(use_gpu=False):\n        params = {\n            'objective': 'multiclass',\n            'metric': 'multi_logloss',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'random_state': 42,\n            'verbose': -1,\n            'min_data_in_leaf': 100,\n            'min_sum_hessian_in_leaf': 1e-2\n        }\n        if use_gpu:\n            params['device'] = 'gpu'\n            print(\"  LightGBM: GPU mode activated\")\n        else:\n            params['device'] = 'cpu'\n            print(\"  LightGBM: CPU mode\")\n        return params\n    \n    @staticmethod\n    def get_xgboost_params(use_gpu=False):\n        params = {\n            'objective': 'multi:softprob',\n            'eval_metric': 'mlogloss',\n            'max_depth': 6,\n            'learning_rate': 0.05,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'random_state': 42,\n            'verbosity': 1\n        }\n        if use_gpu:\n            params['device'] = 'cuda'\n            print(\"  XGBoost: GPU mode activated\")\n        else:\n            params['device'] = 'cpu'\n            print(\"  XGBoost: CPU mode\")\n        return params\n\nconfig = Config()\nprint(\"\\nConfiguration loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:19:43.189947Z","iopub.execute_input":"2026-01-11T14:19:43.190242Z","iopub.status.idle":"2026-01-11T14:19:43.198866Z","shell.execute_reply.started":"2026-01-11T14:19:43.190219Z","shell.execute_reply":"2026-01-11T14:19:43.197987Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nConfiguration loaded successfully.\n","output_type":"stream"}],"execution_count":18},{"id":"3134a05c","cell_type":"markdown","source":"## 2. Data Loading & Validation","metadata":{}},{"id":"37144dad","cell_type":"code","source":"def load_data():\n    print(\"Loading data...\")\n    files = ['train.csv', 'test.csv', 'sample_submission.csv']\n    data = {}\n    \n    for file in tqdm(files, desc=\"Loading files\"):\n        data[file.replace('.csv', '')] = pd.read_csv(config.DATA_PATH + file)\n    \n    train = data['train']\n    test = data['test']\n    sample_submission = data['sample_submission']\n    \n    print(f\"Train shape: {train.shape}\")\n    print(f\"Test shape: {test.shape}\")\n    print(f\"Sample submission shape: {sample_submission.shape}\")\n    \n    if config.TARGET_COL in train.columns:\n        print(f\"\\nTarget distribution:\")\n        print(train[config.TARGET_COL].value_counts())\n    \n    return train, test, sample_submission\n\ntrain, test, sample_submission = load_data()","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:19:46.264664Z","iopub.execute_input":"2026-01-11T14:19:46.264973Z","iopub.status.idle":"2026-01-11T14:20:58.836358Z","shell.execute_reply.started":"2026-01-11T14:19:46.264948Z","shell.execute_reply":"2026-01-11T14:20:58.835515Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80bb5184ff3f4565a9f782edd8a4d8c6"}},"metadata":{}},{"name":"stdout","text":"Train shape: (8000000, 25)\nTest shape: (4000000, 24)\nSample submission shape: (4000000, 2)\n\nTarget distribution:\nTrip_Label\nPerfect_Trip         4397607\nSafety_Violation     1601595\nNavigation_Issue      801790\nService_Complaint     798695\nFraud_Indication      400313\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":19},{"id":"521729c3-ef74-4670-b72a-05ecaa538560","cell_type":"code","source":"train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T14:23:13.994165Z","iopub.execute_input":"2026-01-11T14:23:13.994946Z","iopub.status.idle":"2026-01-11T14:23:14.029306Z","shell.execute_reply.started":"2026-01-11T14:23:13.994909Z","shell.execute_reply":"2026-01-11T14:23:14.028594Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"               Trip_ID            Timestamp  Pickup_Lat  Pickup_Long  \\\n0        TRIP-00383699                  NaN   -6.171481   106.890717   \n1        TRIP-11839677  2024-05-17 00:47:56         NaN   106.962936   \n2        TRIP-00401267  2024-01-05 15:27:46   -6.103016   106.716995   \n3        TRIP-07296718  2024-03-25 10:51:57   -6.128631          NaN   \n4        TRIP-10098925  2024-04-26 21:15:24         NaN   106.930359   \n...                ...                  ...         ...          ...   \n7999995  TRIP-01353088                  NaN   -6.112390   107.044022   \n7999996  TRIP-02587105                  NaN         NaN   106.834473   \n7999997  TRIP-03695100  2024-02-12 18:24:59   -6.312129   106.750740   \n7999998  TRIP-05040403  2024-02-28 08:06:42   -6.239998   106.763947   \n7999999  TRIP-06982866                  NaN   -6.183537          NaN   \n\n         Dropoff_Lat  Dropoff_Long  GPS_Accuracy_M  Distance_KM  \\\n0                NaN    106.961227        7.602805    21.586725   \n1          -6.317811    106.936981        3.567463     2.900550   \n2          -6.002158    106.614403       12.665372    16.015038   \n3          -6.025428    106.972656       24.973850    11.497622   \n4          -5.958007           NaN       13.835665    19.666645   \n...              ...           ...             ...          ...   \n7999995    -6.226453    106.919487       21.854881    18.799213   \n7999996    -6.170597    106.831779        3.460803     2.883191   \n7999997          NaN           NaN       11.436401    16.256813   \n7999998    -6.318290    106.835426       18.077909     0.001180   \n7999999    -6.367938    106.879356             NaN    21.027411   \n\n         Est_Price_IDR  Surge_Multiplier   Accel_X   Accel_Y    Accel_Z  \\\n0        328518.281250          3.093531 -3.637907  0.304190        NaN   \n1         57636.167969               NaN  0.398592  0.595000   9.918528   \n2        141390.171875          1.726858 -0.125994  0.424621   9.867769   \n3        169474.281250          3.031589  0.656659 -0.211875   9.891047   \n4        102339.031250          1.055553  1.105691  0.245058   9.658218   \n...                ...               ...       ...       ...        ...   \n7999995  109668.179688          1.143535 -2.939687 -0.132234  10.060896   \n7999996   75433.882812          3.125755       NaN  1.179822   9.997362   \n7999997   88926.500000          1.245230       NaN       NaN        NaN   \n7999998   20404.679688          1.699778 -1.353521  0.398377   9.948943   \n7999999  208542.640625               NaN  0.717243 -0.040581   9.931121   \n\n           Gyro_Z     Pickup_Zone    Dropoff_Zone              Device_FP  \\\n0        0.235675  JKT-ZONE-06399  JKT-ZONE-03083    Apple-iPhone-7-v3.0   \n1        0.105604  JKT-ZONE-18157  JKT-ZONE-00734       Oppo-Reno-6-v4.0   \n2       -0.034304  JKT-ZONE-08919  JKT-ZONE-17755  Samsung-iPhone-5-v7.0   \n3       -0.076040  JKT-ZONE-02891  JKT-ZONE-16716       Vivo-Hot-10-v7.0   \n4       -0.120996  JKT-ZONE-04617  JKT-ZONE-11542       Apple-Hot-8-v4.0   \n...           ...             ...             ...                    ...   \n7999995  0.676360  JKT-ZONE-03505  JKT-ZONE-02747     Oppo-iPhone-6-v6.0   \n7999996 -0.122138             NaN  JKT-ZONE-07653     Apple-Reno-11-v5.0   \n7999997  0.126295  JKT-ZONE-05303  JKT-ZONE-10856     Apple-Reno-11-v6.0   \n7999998  0.241210             NaN  JKT-ZONE-12772     Oppo-iPhone-6-v3.0   \n7999999  0.039140             NaN  JKT-ZONE-03068     Apple-Reno-11-v8.0   \n\n         Promo_Code       Car_Model Payment_Method Weather Traffic  \\\n0          PCP6AQAY      Honda Brio           Cash    Rain  Lancar   \n1        NO_VOUCHER             NaN    Credit_Card  Cloudy  Lancar   \n2        NO_VOUCHER   Toyota Avanza            NaN   Clear     NaN   \n3               NaN      Honda Brio            OVO   Clear   Padat   \n4        NO_VOUCHER      Honda Brio    Credit_Card   Storm  Lancar   \n...             ...             ...            ...     ...     ...   \n7999995    V8MIQRKA      Honda Brio    Credit_Card  Cloudy  Lancar   \n7999996  NO_VOUCHER  Daihatsu Sigra          Gopay   Storm   Padat   \n7999997  NO_VOUCHER             NaN    Credit_Card  Cloudy     NaN   \n7999998  NO_VOUCHER      Honda Brio            NaN   Storm   Padat   \n7999999    JXX5IETA      Honda Brio            OVO   Storm     NaN   \n\n        Battery_Level Signal_Strength         Trip_Label  \n0                 14%              3G   Safety_Violation  \n1                 25%              3G       Perfect_Trip  \n2                 46%              3G       Perfect_Trip  \n3                 NaN            Edge       Perfect_Trip  \n4                  5%              3G       Perfect_Trip  \n...               ...             ...                ...  \n7999995           22%            Edge   Safety_Violation  \n7999996           NaN            Edge       Perfect_Trip  \n7999997           NaN            Edge  Service_Complaint  \n7999998            6%              3G   Fraud_Indication  \n7999999           40%            Edge       Perfect_Trip  \n\n[8000000 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Trip_ID</th>\n      <th>Timestamp</th>\n      <th>Pickup_Lat</th>\n      <th>Pickup_Long</th>\n      <th>Dropoff_Lat</th>\n      <th>Dropoff_Long</th>\n      <th>GPS_Accuracy_M</th>\n      <th>Distance_KM</th>\n      <th>Est_Price_IDR</th>\n      <th>Surge_Multiplier</th>\n      <th>Accel_X</th>\n      <th>Accel_Y</th>\n      <th>Accel_Z</th>\n      <th>Gyro_Z</th>\n      <th>Pickup_Zone</th>\n      <th>Dropoff_Zone</th>\n      <th>Device_FP</th>\n      <th>Promo_Code</th>\n      <th>Car_Model</th>\n      <th>Payment_Method</th>\n      <th>Weather</th>\n      <th>Traffic</th>\n      <th>Battery_Level</th>\n      <th>Signal_Strength</th>\n      <th>Trip_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRIP-00383699</td>\n      <td>NaN</td>\n      <td>-6.171481</td>\n      <td>106.890717</td>\n      <td>NaN</td>\n      <td>106.961227</td>\n      <td>7.602805</td>\n      <td>21.586725</td>\n      <td>328518.281250</td>\n      <td>3.093531</td>\n      <td>-3.637907</td>\n      <td>0.304190</td>\n      <td>NaN</td>\n      <td>0.235675</td>\n      <td>JKT-ZONE-06399</td>\n      <td>JKT-ZONE-03083</td>\n      <td>Apple-iPhone-7-v3.0</td>\n      <td>PCP6AQAY</td>\n      <td>Honda Brio</td>\n      <td>Cash</td>\n      <td>Rain</td>\n      <td>Lancar</td>\n      <td>14%</td>\n      <td>3G</td>\n      <td>Safety_Violation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRIP-11839677</td>\n      <td>2024-05-17 00:47:56</td>\n      <td>NaN</td>\n      <td>106.962936</td>\n      <td>-6.317811</td>\n      <td>106.936981</td>\n      <td>3.567463</td>\n      <td>2.900550</td>\n      <td>57636.167969</td>\n      <td>NaN</td>\n      <td>0.398592</td>\n      <td>0.595000</td>\n      <td>9.918528</td>\n      <td>0.105604</td>\n      <td>JKT-ZONE-18157</td>\n      <td>JKT-ZONE-00734</td>\n      <td>Oppo-Reno-6-v4.0</td>\n      <td>NO_VOUCHER</td>\n      <td>NaN</td>\n      <td>Credit_Card</td>\n      <td>Cloudy</td>\n      <td>Lancar</td>\n      <td>25%</td>\n      <td>3G</td>\n      <td>Perfect_Trip</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRIP-00401267</td>\n      <td>2024-01-05 15:27:46</td>\n      <td>-6.103016</td>\n      <td>106.716995</td>\n      <td>-6.002158</td>\n      <td>106.614403</td>\n      <td>12.665372</td>\n      <td>16.015038</td>\n      <td>141390.171875</td>\n      <td>1.726858</td>\n      <td>-0.125994</td>\n      <td>0.424621</td>\n      <td>9.867769</td>\n      <td>-0.034304</td>\n      <td>JKT-ZONE-08919</td>\n      <td>JKT-ZONE-17755</td>\n      <td>Samsung-iPhone-5-v7.0</td>\n      <td>NO_VOUCHER</td>\n      <td>Toyota Avanza</td>\n      <td>NaN</td>\n      <td>Clear</td>\n      <td>NaN</td>\n      <td>46%</td>\n      <td>3G</td>\n      <td>Perfect_Trip</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRIP-07296718</td>\n      <td>2024-03-25 10:51:57</td>\n      <td>-6.128631</td>\n      <td>NaN</td>\n      <td>-6.025428</td>\n      <td>106.972656</td>\n      <td>24.973850</td>\n      <td>11.497622</td>\n      <td>169474.281250</td>\n      <td>3.031589</td>\n      <td>0.656659</td>\n      <td>-0.211875</td>\n      <td>9.891047</td>\n      <td>-0.076040</td>\n      <td>JKT-ZONE-02891</td>\n      <td>JKT-ZONE-16716</td>\n      <td>Vivo-Hot-10-v7.0</td>\n      <td>NaN</td>\n      <td>Honda Brio</td>\n      <td>OVO</td>\n      <td>Clear</td>\n      <td>Padat</td>\n      <td>NaN</td>\n      <td>Edge</td>\n      <td>Perfect_Trip</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRIP-10098925</td>\n      <td>2024-04-26 21:15:24</td>\n      <td>NaN</td>\n      <td>106.930359</td>\n      <td>-5.958007</td>\n      <td>NaN</td>\n      <td>13.835665</td>\n      <td>19.666645</td>\n      <td>102339.031250</td>\n      <td>1.055553</td>\n      <td>1.105691</td>\n      <td>0.245058</td>\n      <td>9.658218</td>\n      <td>-0.120996</td>\n      <td>JKT-ZONE-04617</td>\n      <td>JKT-ZONE-11542</td>\n      <td>Apple-Hot-8-v4.0</td>\n      <td>NO_VOUCHER</td>\n      <td>Honda Brio</td>\n      <td>Credit_Card</td>\n      <td>Storm</td>\n      <td>Lancar</td>\n      <td>5%</td>\n      <td>3G</td>\n      <td>Perfect_Trip</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7999995</th>\n      <td>TRIP-01353088</td>\n      <td>NaN</td>\n      <td>-6.112390</td>\n      <td>107.044022</td>\n      <td>-6.226453</td>\n      <td>106.919487</td>\n      <td>21.854881</td>\n      <td>18.799213</td>\n      <td>109668.179688</td>\n      <td>1.143535</td>\n      <td>-2.939687</td>\n      <td>-0.132234</td>\n      <td>10.060896</td>\n      <td>0.676360</td>\n      <td>JKT-ZONE-03505</td>\n      <td>JKT-ZONE-02747</td>\n      <td>Oppo-iPhone-6-v6.0</td>\n      <td>V8MIQRKA</td>\n      <td>Honda Brio</td>\n      <td>Credit_Card</td>\n      <td>Cloudy</td>\n      <td>Lancar</td>\n      <td>22%</td>\n      <td>Edge</td>\n      <td>Safety_Violation</td>\n    </tr>\n    <tr>\n      <th>7999996</th>\n      <td>TRIP-02587105</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>106.834473</td>\n      <td>-6.170597</td>\n      <td>106.831779</td>\n      <td>3.460803</td>\n      <td>2.883191</td>\n      <td>75433.882812</td>\n      <td>3.125755</td>\n      <td>NaN</td>\n      <td>1.179822</td>\n      <td>9.997362</td>\n      <td>-0.122138</td>\n      <td>NaN</td>\n      <td>JKT-ZONE-07653</td>\n      <td>Apple-Reno-11-v5.0</td>\n      <td>NO_VOUCHER</td>\n      <td>Daihatsu Sigra</td>\n      <td>Gopay</td>\n      <td>Storm</td>\n      <td>Padat</td>\n      <td>NaN</td>\n      <td>Edge</td>\n      <td>Perfect_Trip</td>\n    </tr>\n    <tr>\n      <th>7999997</th>\n      <td>TRIP-03695100</td>\n      <td>2024-02-12 18:24:59</td>\n      <td>-6.312129</td>\n      <td>106.750740</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11.436401</td>\n      <td>16.256813</td>\n      <td>88926.500000</td>\n      <td>1.245230</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.126295</td>\n      <td>JKT-ZONE-05303</td>\n      <td>JKT-ZONE-10856</td>\n      <td>Apple-Reno-11-v6.0</td>\n      <td>NO_VOUCHER</td>\n      <td>NaN</td>\n      <td>Credit_Card</td>\n      <td>Cloudy</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Edge</td>\n      <td>Service_Complaint</td>\n    </tr>\n    <tr>\n      <th>7999998</th>\n      <td>TRIP-05040403</td>\n      <td>2024-02-28 08:06:42</td>\n      <td>-6.239998</td>\n      <td>106.763947</td>\n      <td>-6.318290</td>\n      <td>106.835426</td>\n      <td>18.077909</td>\n      <td>0.001180</td>\n      <td>20404.679688</td>\n      <td>1.699778</td>\n      <td>-1.353521</td>\n      <td>0.398377</td>\n      <td>9.948943</td>\n      <td>0.241210</td>\n      <td>NaN</td>\n      <td>JKT-ZONE-12772</td>\n      <td>Oppo-iPhone-6-v3.0</td>\n      <td>NO_VOUCHER</td>\n      <td>Honda Brio</td>\n      <td>NaN</td>\n      <td>Storm</td>\n      <td>Padat</td>\n      <td>6%</td>\n      <td>3G</td>\n      <td>Fraud_Indication</td>\n    </tr>\n    <tr>\n      <th>7999999</th>\n      <td>TRIP-06982866</td>\n      <td>NaN</td>\n      <td>-6.183537</td>\n      <td>NaN</td>\n      <td>-6.367938</td>\n      <td>106.879356</td>\n      <td>NaN</td>\n      <td>21.027411</td>\n      <td>208542.640625</td>\n      <td>NaN</td>\n      <td>0.717243</td>\n      <td>-0.040581</td>\n      <td>9.931121</td>\n      <td>0.039140</td>\n      <td>NaN</td>\n      <td>JKT-ZONE-03068</td>\n      <td>Apple-Reno-11-v8.0</td>\n      <td>JXX5IETA</td>\n      <td>Honda Brio</td>\n      <td>OVO</td>\n      <td>Storm</td>\n      <td>NaN</td>\n      <td>40%</td>\n      <td>Edge</td>\n      <td>Perfect_Trip</td>\n    </tr>\n  </tbody>\n</table>\n<p>8000000 rows Ã— 25 columns</p>\n</div>"},"metadata":{}}],"execution_count":22},{"id":"74d1d42f","cell_type":"code","source":"def optimize_memory(df):\n    print(f\"Optimizing memory for dataframe with {len(df.columns)} columns...\")\n    for col in tqdm(df.columns, desc=\"Optimizing columns\"):\n        col_type = df[col].dtype\n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n    return df\n\nprint(\"Optimizing memory...\")\ntrain = optimize_memory(train)\ntest = optimize_memory(test)\nprint(\"Memory optimization completed.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:22:01.932416Z","iopub.execute_input":"2026-01-11T14:22:01.932748Z","iopub.status.idle":"2026-01-11T14:22:02.534228Z","shell.execute_reply.started":"2026-01-11T14:22:01.932722Z","shell.execute_reply":"2026-01-11T14:22:02.533321Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Optimizing memory...\nOptimizing memory for dataframe with 25 columns...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Optimizing columns:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72cf4c98924440ca39c4bf3f488dbc1"}},"metadata":{}},{"name":"stdout","text":"Optimizing memory for dataframe with 24 columns...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Optimizing columns:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc1fba80880749c5a093a8c910d3ac45"}},"metadata":{}},{"name":"stdout","text":"Memory optimization completed.\n","output_type":"stream"}],"execution_count":20},{"id":"2b7bf178","cell_type":"markdown","source":"## 3. Feature Engineering","metadata":{}},{"id":"bb072add","cell_type":"code","source":"def haversine_distance(lat1, lon1, lat2, lon2):\n    R = 6371\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    delta_lat = np.radians(lat2 - lat1)\n    delta_lon = np.radians(lon2 - lon1)\n    \n    a = np.sin(delta_lat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(delta_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    \n    return R * c\n\ndef engineer_features(df, is_train=True):\n    print(f\"Engineering features for {'train' if is_train else 'test'} set...\")\n    df = df.copy()\n    \n    if config.USE_TEMPORAL and 'Timestamp' in df.columns:\n        print(\"  - Creating temporal features...\")\n        \n        initial_rows = len(df)\n        missing_ts = df['Timestamp'].isnull().sum()\n        \n        if missing_ts > 0:\n            print(f\"    WARNING: {missing_ts} rows with missing Timestamp\")\n            if is_train:\n                df = df[df['Timestamp'].notna()].copy()\n                print(f\"    Dropped {initial_rows - len(df)} rows from training data\")\n            else:\n                print(f\"    Filling missing timestamps in test set\")\n        \n        df['Timestamp_parsed'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n        \n        nat_count = df['Timestamp_parsed'].isnull().sum()\n        if nat_count > 0:\n            print(f\"    WARNING: {nat_count} invalid timestamps detected\")\n            if is_train:\n                df = df[df['Timestamp_parsed'].notna()].copy()\n            else:\n                df['Timestamp_parsed'] = df['Timestamp_parsed'].fillna(pd.Timestamp('2024-01-01'))\n        \n        df['Hour'] = df['Timestamp_parsed'].dt.hour.astype(np.int8)\n        df['DayOfWeek'] = df['Timestamp_parsed'].dt.dayofweek.astype(np.int8)\n        df['Month'] = df['Timestamp_parsed'].dt.month.astype(np.int8)\n        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(np.int8)\n        df['IsRushHour'] = ((df['Hour'] >= 7) & (df['Hour'] <= 9) | \n                            (df['Hour'] >= 17) & (df['Hour'] <= 19)).astype(np.int8)\n        df['IsLateNight'] = ((df['Hour'] >= 22) | (df['Hour'] <= 5)).astype(np.int8)\n        df.drop('Timestamp_parsed', axis=1, inplace=True)\n    \n    if config.USE_DISTANCE:\n        print(\"  - Creating distance features...\")\n        if all(col in df.columns for col in ['Pickup_Lat', 'Pickup_Long', 'Dropoff_Lat', 'Dropoff_Long']):\n            df['Haversine_Distance'] = haversine_distance(\n                df['Pickup_Lat'], df['Pickup_Long'],\n                df['Dropoff_Lat'], df['Dropoff_Long']\n            )\n            \n            if 'Distance_KM' in df.columns:\n                df['Distance_Ratio'] = df['Distance_KM'] / (df['Haversine_Distance'] + 1e-6)\n                df['Distance_Difference'] = np.abs(df['Distance_KM'] - df['Haversine_Distance'])\n        \n        if 'Pickup_Zone' in df.columns and 'Dropoff_Zone' in df.columns:\n            df['Is_Same_Zone'] = (df['Pickup_Zone'] == df['Dropoff_Zone']).astype(np.int8)\n    \n    if config.USE_SENSOR_AGG:\n        print(\"  - Creating sensor aggregation features...\")\n        if all(col in df.columns for col in ['Accel_X', 'Accel_Y', 'Accel_Z']):\n            df['Accel_Magnitude'] = np.sqrt(df['Accel_X']**2 + df['Accel_Y']**2 + df['Accel_Z']**2)\n            df['Accel_Max'] = df[['Accel_X', 'Accel_Y', 'Accel_Z']].max(axis=1)\n            df['Accel_Min'] = df[['Accel_X', 'Accel_Y', 'Accel_Z']].min(axis=1)\n            df['Accel_Std'] = df[['Accel_X', 'Accel_Y', 'Accel_Z']].std(axis=1)\n            df['Accel_Range'] = df['Accel_Max'] - df['Accel_Min']\n        \n        if 'Gyro_Z' in df.columns:\n            df['Gyro_Abs'] = np.abs(df['Gyro_Z'])\n    \n    if config.USE_ECONOMIC:\n        print(\"  - Creating economic features...\")\n        if 'Est_Price_IDR' in df.columns and 'Distance_KM' in df.columns:\n            df['Price_per_KM'] = df['Est_Price_IDR'] / (df['Distance_KM'] + 1e-6)\n        \n        if 'Promo_Code' in df.columns:\n            df['Has_Promo'] = (df['Promo_Code'].notna()).astype(np.int8)\n        \n        if 'Surge_Multiplier' in df.columns:\n            df['Surge_Category'] = pd.cut(df['Surge_Multiplier'], \n                                          bins=[0, 1, 1.5, 2, 10], \n                                          labels=[0, 1, 2, 3]).astype(np.int8)\n    \n    if config.USE_INTERACTION:\n        print(\"  - Creating interaction features...\")\n        if 'Surge_Multiplier' in df.columns and 'Hour' in df.columns:\n            df['Surge_Hour_Interaction'] = df['Surge_Multiplier'] * df['Hour']\n        \n        if 'Distance_KM' in df.columns and 'Traffic' in df.columns:\n            traffic_map = {'Light': 1, 'Moderate': 2, 'Heavy': 3}\n            df['Traffic_Numeric'] = df['Traffic'].map(traffic_map).fillna(0).astype(np.int8)\n            df['Distance_Traffic'] = df['Distance_KM'] * df['Traffic_Numeric']\n    \n    print(f\"Feature engineering completed. Shape: {df.shape}\")\n    return df\n\ntrain = engineer_features(train, is_train=True)\ntest = engineer_features(test, is_train=False)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:26:11.746373Z","iopub.execute_input":"2026-01-11T14:26:11.747093Z","iopub.status.idle":"2026-01-11T14:26:29.475786Z","shell.execute_reply.started":"2026-01-11T14:26:11.747064Z","shell.execute_reply":"2026-01-11T14:26:29.475135Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Engineering features for train set...\n  - Creating temporal features...\n    WARNING: 1637893 rows with missing Timestamp\n    Dropped 1637893 rows from training data\n  - Creating distance features...\n  - Creating sensor aggregation features...\n  - Creating interaction features...\nFeature engineering completed. Shape: (6362107, 44)\nEngineering features for test set...\n  - Creating temporal features...\n    WARNING: 163030 rows with missing Timestamp\n    Filling missing timestamps in test set\n    WARNING: 163030 invalid timestamps detected\n  - Creating distance features...\n  - Creating sensor aggregation features...\n  - Creating interaction features...\nFeature engineering completed. Shape: (4000000, 43)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"1329"},"metadata":{}}],"execution_count":23},{"id":"2d23f1e9","cell_type":"markdown","source":"## 4. Preprocessing","metadata":{}},{"id":"e9713440","cell_type":"code","source":"def encode_categorical_frequency(train, test, categorical_cols):\n    \"\"\"\n    Encodes based on value frequency in training data\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"FREQUENCY ENCODING\")\n    print(\"=\"*80)\n    \n    encoders = {}\n    \n    for col in tqdm(categorical_cols, desc=\"Frequency encoding\"):\n        freq_map = train[col].value_counts(dropna=False).to_dict()\n        \n        train[col] = train[col].map(freq_map).fillna(0).astype(np.int32)\n        test[col] = test[col].map(freq_map).fillna(0).astype(np.int32)\n        \n        encoders[col] = {\n            'type': 'frequency',\n            'unique_values': len(freq_map),\n            'unseen_in_test': (test[col] == 0).sum()\n        }\n    \n    print(f\"\\nEncoded {len(categorical_cols)} categorical features\")\n    return train, test, encoders","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:26:34.090698Z","iopub.execute_input":"2026-01-11T14:26:34.091409Z","iopub.status.idle":"2026-01-11T14:26:34.096628Z","shell.execute_reply.started":"2026-01-11T14:26:34.091382Z","shell.execute_reply":"2026-01-11T14:26:34.096059Z"},"trusted":true},"outputs":[],"execution_count":24},{"id":"b7bb1a79","cell_type":"code","source":"def encode_categorical_target(train, test, categorical_cols, y_train, smoothing=10):\n    \"\"\"\n    Encodes based on target mean, with smoothing to prevent overfitting\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"TARGET ENCODING\")\n    print(\"=\"*80)\n    \n    encoders = {}\n    global_mean = y_train.mean()\n    \n    for col in tqdm(categorical_cols, desc=\"Target encoding\"):\n        temp_df = pd.DataFrame({col: train[col], 'target': y_train})\n        \n        agg = temp_df.groupby(col)['target'].agg(['mean', 'count'])\n        smoothed_mean = (agg['mean'] * agg['count'] + global_mean * smoothing) / (agg['count'] + smoothing)\n        \n        encoding_map = smoothed_mean.to_dict()\n        \n        train[col] = train[col].map(encoding_map).fillna(global_mean).astype(np.float32)\n        test[col] = test[col].map(encoding_map).fillna(global_mean).astype(np.float32)\n        \n        encoders[col] = {\n            'type': 'target',\n            'unique_values': len(encoding_map),\n            'global_mean': global_mean\n        }\n    \n    print(f\"\\nEncoded {len(categorical_cols)} categorical features\")\n    return train, test, encoders","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:26:35.890479Z","iopub.execute_input":"2026-01-11T14:26:35.891310Z","iopub.status.idle":"2026-01-11T14:26:35.897198Z","shell.execute_reply.started":"2026-01-11T14:26:35.891282Z","shell.execute_reply":"2026-01-11T14:26:35.896497Z"},"trusted":true},"outputs":[],"execution_count":25},{"id":"196940cd","cell_type":"code","source":"def encode_categorical_label_optimized(train, test, categorical_cols):\n    \"\"\"\n    Uses map() for vectorized operations\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"LABEL ENCODING \")\n    print(\"=\"*80)\n    \n    encoders = {}\n    \n    for col in tqdm(categorical_cols, desc=\"Label encoding\"):\n        train[col] = train[col].astype(str)\n        test[col] = test[col].astype(str)\n        \n        le = LabelEncoder()\n        train[col] = le.fit_transform(train[col])\n        \n        mapping = {label: idx for idx, label in enumerate(le.classes_)}\n        test[col] = test[col].map(mapping).fillna(-1).astype(np.int32)\n        \n        encoders[col] = {\n            'type': 'label',\n            'encoder': le,\n            'unique_values': len(le.classes_),\n            'unseen_in_test': (test[col] == -1).sum()\n        }\n    \n    print(f\"\\nEncoded {len(categorical_cols)} categorical features\")\n    return train, test, encoders","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:26:37.803985Z","iopub.execute_input":"2026-01-11T14:26:37.804759Z","iopub.status.idle":"2026-01-11T14:26:37.810473Z","shell.execute_reply.started":"2026-01-11T14:26:37.804722Z","shell.execute_reply":"2026-01-11T14:26:37.809787Z"},"trusted":true},"outputs":[],"execution_count":26},{"id":"445248f0","cell_type":"code","source":"def preprocess_data(train, test, encoding_method='frequency'):\n    \"\"\"\n    Main preprocessing pipeline\n    \n    Parameters:\n    -----------\n    train : pd.DataFrame\n        Training dataset\n    test : pd.DataFrame\n        Test dataset\n    encoding_method : str\n        Encoding method to use: 'frequency', 'target', or 'label'\n    Returns:\n    --------\n    X_train, X_test, y_train, le_target, encoders\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"DATA PREPROCESSING PIPELINE\")\n    print(\"=\"*80)\n    print(f\"Encoding method: {encoding_method.upper()}\")\n    \n    cols_to_drop = [config.ID_COL, 'Timestamp']\n    if config.TARGET_COL in train.columns:\n        y = train[config.TARGET_COL].copy()\n        cols_to_drop.append(config.TARGET_COL)\n    else:\n        y = None\n    \n    cols_to_drop = [col for col in cols_to_drop if col in train.columns]\n    X_train = train.drop(cols_to_drop, axis=1).copy()\n    X_test = test.drop([col for col in cols_to_drop if col in test.columns], axis=1).copy()\n    \n    print(f\"\\nInitial shapes:\")\n    print(f\"  X_train: {X_train.shape}\")\n    print(f\"  X_test: {X_test.shape}\")\n    \n    print(f\"\\nMissing values before imputation:\")\n    train_missing = X_train.isnull().sum()\n    if train_missing.sum() > 0:\n        print(train_missing[train_missing > 0])\n    else:\n        print(\"  No missing values found\")\n    \n    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n    \n    print(f\"\\nFeature types detected:\")\n    print(f\"  Numeric features: {len(numeric_cols)}\")\n    print(f\"  Categorical features: {len(categorical_cols)}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 1: Missing Value Imputation\")\n    print(\"=\"*80)\n    \n    for col in tqdm(numeric_cols, desc=\"Imputing numeric features\"):\n        if X_train[col].isnull().sum() > 0:\n            median_val = X_train[col].median()\n            X_train[col].fillna(median_val, inplace=True)\n            X_test[col].fillna(median_val, inplace=True)\n    \n    for col in tqdm(categorical_cols, desc=\"Imputing categorical features\"):\n        if X_train[col].isnull().sum() > 0:\n            X_train[col].fillna('Unknown', inplace=True)\n            X_test[col].fillna('Unknown', inplace=True)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 2: Outlier Clipping\")\n    print(\"=\"*80)\n    \n    for col in tqdm(numeric_cols, desc=\"Clipping outliers\"):\n        q99 = X_train[col].quantile(0.99)\n        q01 = X_train[col].quantile(0.01)\n        X_train[col] = X_train[col].clip(q01, q99)\n        X_test[col] = X_test[col].clip(q01, q99)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 3: Categorical Encoding\")\n    print(\"=\"*80)\n    \n    if len(categorical_cols) > 0:\n        if encoding_method == 'frequency':\n            X_train, X_test, encoders = encode_categorical_frequency(\n                X_train, X_test, categorical_cols\n            )\n        elif encoding_method == 'target':\n            if y is None:\n                raise ValueError(\"Target encoding requires target variable\")\n            le_target_temp = LabelEncoder()\n            y_temp = le_target_temp.fit_transform(y)\n            X_train, X_test, encoders = encode_categorical_target(\n                X_train, X_test, categorical_cols, y_temp\n            )\n        elif encoding_method == 'label':\n            X_train, X_test, encoders = encode_categorical_label_optimized(\n                X_train, X_test, categorical_cols\n            )\n        else:\n            raise ValueError(f\"Unknown encoding method: {encoding_method}\")\n    else:\n        encoders = {}\n        print(\"No categorical features to encode\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 4: Target Encoding\")\n    print(\"=\"*80)\n    \n    if y is not None:\n        le_target = LabelEncoder()\n        y_encoded = le_target.fit_transform(y)\n        print(f\"\\nTarget classes encoded:\")\n        for i, label in enumerate(le_target.classes_):\n            count = (y_encoded == i).sum()\n            print(f\"  {i}: {label:20s} - {count:,} samples ({count/len(y_encoded)*100:.2f}%)\")\n    else:\n        y_encoded = None\n        le_target = None\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PREPROCESSING COMPLETED\")\n    print(\"=\"*80)\n    print(f\"Final shapes:\")\n    print(f\"  X_train: {X_train.shape}\")\n    print(f\"  X_test: {X_test.shape}\")\n    if y_encoded is not None:\n        print(f\"  y_train: {y_encoded.shape}\")\n    print(\"=\"*80)\n    \n    return X_train, X_test, y_encoded, le_target, encoders\n\nX_train, X_test, y_train, le_target, encoders = preprocess_data(train, test, encoding_method='frequency')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:27:23.049080Z","iopub.execute_input":"2026-01-11T14:27:23.049398Z","iopub.status.idle":"2026-01-11T14:28:06.424072Z","shell.execute_reply.started":"2026-01-11T14:27:23.049367Z","shell.execute_reply":"2026-01-11T14:28:06.423368Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n================================================================================\nDATA PREPROCESSING PIPELINE\n================================================================================\nEncoding method: FREQUENCY\n\nInitial shapes:\n  X_train: (6362107, 41)\n  X_test: (4000000, 41)\n\nMissing values before imputation:\nPickup_Lat                 739164\nPickup_Long                485749\nDropoff_Lat               1522278\nDropoff_Long              1237137\nGPS_Accuracy_M            1195894\nDistance_KM                748217\nEst_Price_IDR              915303\nSurge_Multiplier           487462\nAccel_X                   1279764\nAccel_Y                    859833\nAccel_Z                   1464042\nGyro_Z                     558538\nPickup_Zone               1099320\nDropoff_Zone               608878\nDevice_FP                 1192507\nPromo_Code                 880814\nCar_Model                 1369053\nPayment_Method             907359\nWeather                    409114\nTraffic                   1437555\nBattery_Level             1388653\nSignal_Strength            324901\nHaversine_Distance        3179726\nDistance_Ratio            3553734\nDistance_Difference       3553734\nAccel_Magnitude           2978662\nAccel_Max                   39909\nAccel_Min                   39909\nAccel_Std                  585068\nAccel_Range                 39909\nGyro_Abs                   558538\nSurge_Hour_Interaction     487462\nDistance_Traffic           748217\ndtype: int64\n\nFeature types detected:\n  Numeric features: 31\n  Categorical features: 10\n\n================================================================================\nSTEP 1: Missing Value Imputation\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Imputing numeric features:   0%|          | 0/31 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a20f2377bc394611957fe2d4a36771af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Imputing categorical features:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7066d0f9d014b2ea92f241ff6d3ac17"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nSTEP 2: Outlier Clipping\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Clipping outliers:   0%|          | 0/31 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5661cb7610d74dccac107e2bba8907fe"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nSTEP 3: Categorical Encoding\n================================================================================\n\n================================================================================\nFREQUENCY ENCODING\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Frequency encoding:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cce09cfdf6664a62ac318d16c85ed7cd"}},"metadata":{}},{"name":"stdout","text":"\nEncoded 10 categorical features\n\n================================================================================\nSTEP 4: Target Encoding\n================================================================================\n\nTarget classes encoded:\n  0: Fraud_Indication     - 318,313 samples (5.00%)\n  1: Navigation_Issue     - 637,130 samples (10.01%)\n  2: Perfect_Trip         - 3,498,094 samples (54.98%)\n  3: Safety_Violation     - 1,274,072 samples (20.03%)\n  4: Service_Complaint    - 634,498 samples (9.97%)\n\n================================================================================\nPREPROCESSING COMPLETED\n================================================================================\nFinal shapes:\n  X_train: (6362107, 41)\n  X_test: (4000000, 41)\n  y_train: (6362107,)\n================================================================================\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"78"},"metadata":{}}],"execution_count":28},{"id":"80430d8d","cell_type":"markdown","source":"## 5. Model Training","metadata":{}},{"id":"98307431-beb7-4bcf-9685-ce54a359f37f","cell_type":"code","source":"def macro_f1_eval(preds, dtrain):\n    \"\"\"\n    Custom evaluation function for XGBoost to calculate Macro F1\n    \"\"\"\n    labels = dtrain.get_label()\n    preds_reshaped = preds.reshape(len(labels), -1)\n    pred_labels = np.argmax(preds_reshaped, axis=1)\n    # Calculate macro F1\n    score = f1_score(labels, pred_labels, average='macro')\n    return 'macro_f1', score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T14:33:12.140134Z","iopub.execute_input":"2026-01-11T14:33:12.140795Z","iopub.status.idle":"2026-01-11T14:33:12.144886Z","shell.execute_reply.started":"2026-01-11T14:33:12.140769Z","shell.execute_reply":"2026-01-11T14:33:12.144171Z"}},"outputs":[],"execution_count":30},{"id":"6340dc9b","cell_type":"code","source":"try:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept ImportError:\n    print(\"XGBoost not available. Install with: pip install xgboost\")\n    XGBOOST_AVAILABLE = False\n\ndef train_xgboost(X_train, y_train, X_test, n_folds=5, use_gpu=False):\n    if not XGBOOST_AVAILABLE:\n        return None, None, None\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"Training XGBoost Models\")\n    print(\"=\"*80)\n    \n    params = config.get_xgboost_params(use_gpu=use_gpu)\n    params['num_class'] = len(np.unique(y_train))\n    \n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=config.RANDOM_STATE)\n    \n    oof_predictions = np.zeros((len(X_train), len(np.unique(y_train))))\n    test_predictions = np.zeros((len(X_test), len(np.unique(y_train))))\n    \n    fold_scores = []\n    models = []\n    \n    pbar = tqdm(enumerate(skf.split(X_train, y_train), 1), total=n_folds, desc=\"XGBoost Folds\")\n    for fold, (train_idx, val_idx) in pbar:\n        pbar.set_description(f\"XGBoost Fold {fold}/{n_folds}\")\n        \n        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n        \n        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        dtest = xgb.DMatrix(X_test)\n        \n        model = xgb.train(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            evals=[(dtrain, 'train'), (dval, 'valid')],\n            custom_metric=macro_f1_eval,\n            early_stopping_rounds=50,\n            verbose_eval=100\n        )\n        \n        oof_predictions[val_idx] = model.predict(dval)\n        test_predictions += model.predict(dtest) / n_folds\n        \n        oof_pred_labels = np.argmax(oof_predictions[val_idx], axis=1)\n        fold_score = f1_score(y_val, oof_pred_labels, average='macro')\n        fold_scores.append(fold_score)\n\n        print(f\"\\n{'='*60}\")\n        print(f\"FOLD {fold} SUMMARY:\")\n        print(f\"{'='*60}\")\n        print(f\"  Validation F1 (Macro): {fold_score:.6f}\")\n        print(f\"  Best Iteration: {model.best_iteration}\")\n        print(f\"  Best Score (mlogloss): {model.best_score:.6f}\")\n        \n        # Detailed classification report\n        oof_fold_pred = np.argmax(oof_predictions[val_idx], axis=1)\n        print(f\"\\n  Per-Class F1 Scores:\")\n        from sklearn.metrics import classification_report\n        print(classification_report(y_val, oof_fold_pred, \n                                   target_names=[f\"Class_{i}\" for i in range(len(np.unique(y_train)))],\n                                   digits=4))\n        print(f\"{'='*60}\\n\")\n        \n        pbar.set_postfix({'F1': f'{fold_score:.6f}'})\n        \n        models.append(model)\n        gc.collect()\n    \n    oof_pred_labels = np.argmax(oof_predictions, axis=1)\n    overall_score = f1_score(y_train, oof_pred_labels, average='macro')\n    \n        \n    print(\"\\n\" + \"=\"*80)\n    print(\"XGBOOST TRAINING SUMMARY\")\n    print(\"=\"*80)\n    print(f\"Overall CV Score (Macro F1): {overall_score:.6f}\")\n    print(f\"Standard Deviation: {np.std(fold_scores):.6f}\")\n    print(f\"Min F1 Score: {np.min(fold_scores):.6f}\")\n    print(f\"Max F1 Score: {np.max(fold_scores):.6f}\")\n    print(f\"\\nFold-by-Fold Scores:\")\n    for i, score in enumerate(fold_scores, 1):\n        print(f\"  Fold {i}: {score:.6f}\")\n    \n    # Overall classification report pada OOF predictions\n    print(f\"\\n\" + \"=\"*80)\n    print(\"OVERALL OUT-OF-FOLD PREDICTIONS REPORT\")\n    print(\"=\"*80)\n    print(classification_report(y_train, oof_pred_labels, \n                               target_names=[f\"Class_{i}\" for i in range(len(np.unique(y_train)))],\n                               digits=4))\n    print(\"=\"*80)\n\n    return test_predictions, models, overall_score\n\n\nif XGBOOST_AVAILABLE:\n    xgboost_test_pred, xgboost_models, xgboost_cv_score = train_xgboost(\n        X_train, y_train, X_test,\n        n_folds=config.N_FOLDS,\n        use_gpu=gpu_config['xgboost_gpu']\n    )\nelse:\n    xgboost_test_pred, xgboost_models, xgboost_cv_score = None, None, 0.0","metadata":{"execution":{"iopub.status.busy":"2026-01-11T14:33:16.305450Z","iopub.execute_input":"2026-01-11T14:33:16.306090Z","iopub.status.idle":"2026-01-11T14:37:24.456322Z","shell.execute_reply.started":"2026-01-11T14:33:16.306059Z","shell.execute_reply":"2026-01-11T14:37:24.455780Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n================================================================================\nTraining XGBoost Models\n================================================================================\n  XGBoost: GPU mode activated\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"XGBoost Folds:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38875955a77941dca89fe0c10981345b"}},"metadata":{}},{"name":"stdout","text":"[0]\ttrain-mlogloss:1.40566\ttrain-macro_f1:0.14191\tvalid-mlogloss:1.40566\tvalid-macro_f1:0.14191\n[50]\ttrain-mlogloss:0.69586\ttrain-macro_f1:0.55880\tvalid-mlogloss:0.69582\tvalid-macro_f1:0.55877\n\n============================================================\nFOLD 1 SUMMARY:\n============================================================\n  Validation F1 (Macro): 0.558770\n  Best Iteration: 0\n  Best Score (mlogloss): 0.141908\n\n  Per-Class F1 Scores:\n              precision    recall  f1-score   support\n\n     Class_0     0.9950    0.9709    0.9828     63663\n     Class_1     0.0000    0.0000    0.0000    127426\n     Class_2     0.7223    0.9989    0.8384    699619\n     Class_3     0.9970    0.9495    0.9727    254814\n     Class_4     0.0000    0.0000    0.0000    126900\n\n    accuracy                         0.7880   1272422\n   macro avg     0.5429    0.5839    0.5588   1272422\nweighted avg     0.6466    0.7880    0.7049   1272422\n\n============================================================\n\n[0]\ttrain-mlogloss:1.40570\ttrain-macro_f1:0.14191\tvalid-mlogloss:1.40571\tvalid-macro_f1:0.14191\n[50]\ttrain-mlogloss:0.69581\ttrain-macro_f1:0.55874\tvalid-mlogloss:0.69589\tvalid-macro_f1:0.55866\n\n============================================================\nFOLD 2 SUMMARY:\n============================================================\n  Validation F1 (Macro): 0.558656\n  Best Iteration: 0\n  Best Score (mlogloss): 0.141908\n\n  Per-Class F1 Scores:\n              precision    recall  f1-score   support\n\n     Class_0     0.9950    0.9700    0.9823     63663\n     Class_1     0.0000    0.0000    0.0000    127426\n     Class_2     0.7222    0.9989    0.8383    699619\n     Class_3     0.9971    0.9494    0.9726    254814\n     Class_4     0.0000    0.0000    0.0000    126900\n\n    accuracy                         0.7879   1272422\n   macro avg     0.5428    0.5837    0.5587   1272422\nweighted avg     0.6465    0.7879    0.7049   1272422\n\n============================================================\n\n[0]\ttrain-mlogloss:1.40568\ttrain-macro_f1:0.14191\tvalid-mlogloss:1.40566\tvalid-macro_f1:0.14191\n[50]\ttrain-mlogloss:0.69580\ttrain-macro_f1:0.55871\tvalid-mlogloss:0.69582\tvalid-macro_f1:0.55885\n\n============================================================\nFOLD 3 SUMMARY:\n============================================================\n  Validation F1 (Macro): 0.558848\n  Best Iteration: 0\n  Best Score (mlogloss): 0.141908\n\n  Per-Class F1 Scores:\n              precision    recall  f1-score   support\n\n     Class_0     0.9941    0.9730    0.9834     63662\n     Class_1     0.0000    0.0000    0.0000    127426\n     Class_2     0.7223    0.9988    0.8383    699619\n     Class_3     0.9968    0.9493    0.9725    254815\n     Class_4     0.0000    0.0000    0.0000    126899\n\n    accuracy                         0.7880   1272421\n   macro avg     0.5426    0.5842    0.5588   1272421\nweighted avg     0.6465    0.7880    0.7049   1272421\n\n============================================================\n\n[0]\ttrain-mlogloss:1.40567\ttrain-macro_f1:0.14191\tvalid-mlogloss:1.40566\tvalid-macro_f1:0.14191\n[49]\ttrain-mlogloss:0.69875\ttrain-macro_f1:0.55870\tvalid-mlogloss:0.69871\tvalid-macro_f1:0.55866\n\n============================================================\nFOLD 4 SUMMARY:\n============================================================\n  Validation F1 (Macro): 0.558658\n  Best Iteration: 0\n  Best Score (mlogloss): 0.141908\n\n  Per-Class F1 Scores:\n              precision    recall  f1-score   support\n\n     Class_0     0.9929    0.9716    0.9822     63662\n     Class_1     0.0000    0.0000    0.0000    127426\n     Class_2     0.7223    0.9988    0.8384    699619\n     Class_3     0.9969    0.9498    0.9728    254815\n     Class_4     0.0000    0.0000    0.0000    126899\n\n    accuracy                         0.7880   1272421\n   macro avg     0.5424    0.5840    0.5587   1272421\nweighted avg     0.6465    0.7880    0.7049   1272421\n\n============================================================\n\n[0]\ttrain-mlogloss:1.40565\ttrain-macro_f1:0.14191\tvalid-mlogloss:1.40568\tvalid-macro_f1:0.14191\n[50]\ttrain-mlogloss:0.69573\ttrain-macro_f1:0.55881\tvalid-mlogloss:0.69609\tvalid-macro_f1:0.55870\n\n============================================================\nFOLD 5 SUMMARY:\n============================================================\n  Validation F1 (Macro): 0.558696\n  Best Iteration: 0\n  Best Score (mlogloss): 0.141908\n\n  Per-Class F1 Scores:\n              precision    recall  f1-score   support\n\n     Class_0     0.9954    0.9710    0.9830     63663\n     Class_1     0.0000    0.0000    0.0000    127426\n     Class_2     0.7221    0.9989    0.8382    699618\n     Class_3     0.9968    0.9488    0.9722    254814\n     Class_4     0.0000    0.0000    0.0000    126900\n\n    accuracy                         0.7878   1272421\n   macro avg     0.5429    0.5837    0.5587   1272421\nweighted avg     0.6465    0.7878    0.7048   1272421\n\n============================================================\n\n\n================================================================================\nXGBOOST TRAINING SUMMARY\n================================================================================\nOverall CV Score (Macro F1): 0.558726\nStandard Deviation: 0.000074\nMin F1 Score: 0.558656\nMax F1 Score: 0.558848\n\nFold-by-Fold Scores:\n  Fold 1: 0.558770\n  Fold 2: 0.558656\n  Fold 3: 0.558848\n  Fold 4: 0.558658\n  Fold 5: 0.558696\n\n================================================================================\nOVERALL OUT-OF-FOLD PREDICTIONS REPORT\n================================================================================\n              precision    recall  f1-score   support\n\n     Class_0     0.9945    0.9713    0.9827    318313\n     Class_1     0.0000    0.0000    0.0000    637130\n     Class_2     0.7222    0.9989    0.8383   3498094\n     Class_3     0.9969    0.9494    0.9726   1274072\n     Class_4     0.0000    0.0000    0.0000    634498\n\n    accuracy                         0.7879   6362107\n   macro avg     0.5427    0.5839    0.5587   6362107\nweighted avg     0.6465    0.7879    0.7049   6362107\n\n================================================================================\n","output_type":"stream"}],"execution_count":31},{"id":"69abb317","cell_type":"markdown","source":"## 6. Model Evaluation and Inference","metadata":{}},{"id":"a8a333a8","cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"MODEL EVALUATION\")\nprint(\"=\"*80)\n\nif xgboost_test_pred is not None:\n    print(f\"\\nâœ“ XGBoost Model Successfully Trained\")\n    print(f\"  Cross-Validation Score: {xgboost_cv_score:.6f}\")\n    print(f\"  Model Type: XGBoost with GPU acceleration\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING PREDICTIONS ON TEST SET\")\n    print(\"=\"*80)\n    \n    final_predictions = xgboost_test_pred\n    final_pred_labels = np.argmax(final_predictions, axis=1)\n    \n    print(f\"\\nâœ“ Predictions Generated Successfully\")\n    print(f\"  Total test samples: {len(final_pred_labels):,}\")\n    print(f\"  Prediction shape: {final_predictions.shape}\")\n    print(f\"  Classes predicted: {len(np.unique(final_pred_labels))}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PREDICTION DISTRIBUTION\")\n    print(\"=\"*80)\n    unique, counts = np.unique(final_pred_labels, return_counts=True)\n    for class_idx, count in zip(unique, counts):\n        percentage = (count / len(final_pred_labels)) * 100\n        print(f\"  Class {class_idx}: {count:,} samples ({percentage:.2f}%)\")\n    \nelse:\n    raise ValueError(\"XGBoost model training failed! Cannot generate predictions.\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T14:38:40.991161Z","iopub.execute_input":"2026-01-11T14:38:40.991757Z","iopub.status.idle":"2026-01-11T14:38:41.099107Z","shell.execute_reply.started":"2026-01-11T14:38:40.991731Z","shell.execute_reply":"2026-01-11T14:38:41.098501Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nMODEL EVALUATION\n================================================================================\n\nâœ“ XGBoost Model Successfully Trained\n  Cross-Validation Score: 0.558726\n  Model Type: XGBoost with GPU acceleration\n\n================================================================================\nGENERATING PREDICTIONS ON TEST SET\n================================================================================\n\nâœ“ Predictions Generated Successfully\n  Total test samples: 4,000,000\n  Prediction shape: (4000000, 5)\n  Classes predicted: 3\n\n================================================================================\nPREDICTION DISTRIBUTION\n================================================================================\n  Class 0: 197,554 samples (4.94%)\n  Class 2: 3,012,438 samples (75.31%)\n  Class 3: 790,008 samples (19.75%)\n\n================================================================================\n","output_type":"stream"}],"execution_count":32},{"id":"f5ebc841","cell_type":"markdown","source":"## 7. Generate Submission","metadata":{}},{"id":"ebd660f5","cell_type":"code","source":"def create_submission(test_ids, predictions, le_target, filename='submission.csv'):\n    pred_labels = le_target.inverse_transform(predictions)\n    \n    submission = pd.DataFrame({\n        config.ID_COL: test_ids,\n        config.TARGET_COL: pred_labels\n    })\n    \n    submission.to_csv(filename, index=False)\n    \n    print(f\"\\nSubmission saved to: {filename}\")\n    print(f\"Submission shape: {submission.shape}\")\n    print(f\"\\nPrediction distribution:\")\n    print(submission[config.TARGET_COL].value_counts())\n    \n    return submission\n\ntest_ids = test[config.ID_COL].values\nsubmission = create_submission(test_ids, final_pred_labels, le_target, 'submission.csv')\nsubmission.head(10)","metadata":{},"outputs":[],"execution_count":null},{"id":"e64c9f05","cell_type":"markdown","source":"## 8. Validation & Analysis","metadata":{}},{"id":"18f7d50d","cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"Final Validation Checks\")\nprint(\"=\"*80)\n\nassert submission.shape[0] == test.shape[0], \"Submission size mismatch!\"\nassert submission.columns.tolist() == [config.ID_COL, config.TARGET_COL], \"Column names mismatch!\"\nassert submission[config.TARGET_COL].isnull().sum() == 0, \"Null predictions found!\"\n\nexpected_labels = set(le_target.classes_)\nsubmission_labels = set(submission[config.TARGET_COL].unique())\nassert submission_labels.issubset(expected_labels), \"Invalid labels in submission!\"\n\nprint(\"All validation checks passed!\")\nprint(\"=\"*80)","metadata":{},"outputs":[],"execution_count":null}]}