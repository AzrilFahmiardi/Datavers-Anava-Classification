{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a11a26",
   "metadata": {},
   "source": [
    "# Ride-Hailing Trip Classification\n",
    "\n",
    "Pipeline for XGBoost + CatBoost ensemble classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98543c7e",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef70f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_PATH = '/kaggle/input/ride-hailing-trip-classification-dataset/'\n",
    "    N_FOLDS = 5\n",
    "    RANDOM_STATE = 42\n",
    "    TARGET_COL = 'Trip_Label'\n",
    "    ID_COL = 'Trip_ID'\n",
    "    USE_GPU = True\n",
    "    \n",
    "    XGBOOST_PARAMS = {\n",
    "        \"max_depth\": 7,\n",
    "        \"min_child_weight\": 7,\n",
    "        \"max_delta_step\": 5,\n",
    "        \"gamma\": 0.1737507723343592,\n",
    "        \"learning_rate\": 0.034002141615166376,\n",
    "        \"subsample\": 0.9516920090100376,\n",
    "        \"colsample_bytree\": 0.5993238619880857,\n",
    "        \"colsample_bylevel\": 0.9963742019132593,\n",
    "        \"colsample_bynode\": 0.9228789464520328,\n",
    "        \"reg_alpha\": 1.0572914540483875,\n",
    "        \"reg_lambda\": 2.1685276721731586\n",
    "    }\n",
    "    \n",
    "    CLASS_WEIGHTS = {\n",
    "        0: 1.0,\n",
    "        1: 4.509194597384958,\n",
    "        2: 1.0,\n",
    "        3: 1.0,\n",
    "        4: 4.96455487505969\n",
    "    }\n",
    "    \n",
    "    THRESHOLDS = np.array([2.1188, 1.8953, 1.6760, 1.7246, 1.6181])\n",
    "    \n",
    "    ADVERSARIAL_FEATURES = [\n",
    "        'null_count',\n",
    "        'null_count_temporal',\n",
    "        'null_count_distance',\n",
    "        'null_count_sensor',\n",
    "        'null_count_economic',\n",
    "    ]\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7427b",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    train = pd.read_csv(config.DATA_PATH + 'train.csv')\n",
    "    test = pd.read_csv(config.DATA_PATH + 'test.csv')\n",
    "    \n",
    "    print(f\"Train shape: {train.shape}\")\n",
    "    print(f\"Test shape: {test.shape}\")\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(train[config.TARGET_COL].value_counts())\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = load_data()\n",
    "train_distribution = train[config.TARGET_COL].value_counts(normalize=True).sort_index()\n",
    "test_ids = test[config.ID_COL].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5683c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != 'object':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "train = optimize_memory(train)\n",
    "test = optimize_memory(test)\n",
    "print(\"Memory optimization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd504cf1",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROUTE_FREQ = {}\n",
    "PICKUP_ZONE_FREQ = {}\n",
    "DROPOFF_ZONE_FREQ = {}\n",
    "\n",
    "def compute_bearing(lat1, lon1, lat2, lon2):\n",
    "    lat1, lat2 = np.radians(lat1), np.radians(lat2)\n",
    "    diff = np.radians(lon2 - lon1)\n",
    "    x = np.sin(diff) * np.cos(lat2)\n",
    "    y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(diff)\n",
    "    return np.degrees(np.arctan2(x, y))\n",
    "\n",
    "def engineer_features(df, is_train=True):\n",
    "    global ROUTE_FREQ, PICKUP_ZONE_FREQ, DROPOFF_ZONE_FREQ\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Null count features\n",
    "    df['null_count'] = df.isnull().sum(axis=1)\n",
    "    \n",
    "    temporal_cols = ['Timestamp']\n",
    "    df['null_count_temporal'] = df[[c for c in temporal_cols if c in df.columns]].isnull().sum(axis=1)\n",
    "    \n",
    "    distance_cols = ['Pickup_Lat', 'Pickup_Long', 'Dropoff_Lat', 'Dropoff_Long', 'Distance_KM']\n",
    "    df['null_count_distance'] = df[[c for c in distance_cols if c in df.columns]].isnull().sum(axis=1)\n",
    "    \n",
    "    sensor_cols = ['Accel_X', 'Accel_Y', 'Accel_Z', 'Gyro_Z']\n",
    "    df['null_count_sensor'] = df[[c for c in sensor_cols if c in df.columns]].isnull().sum(axis=1)\n",
    "    \n",
    "    econ_cols = ['Est_Price_IDR', 'Promo_Code', 'Surge_Multiplier']\n",
    "    df['null_count_economic'] = df[[c for c in econ_cols if c in df.columns]].isnull().sum(axis=1)\n",
    "    \n",
    "    # Temporal features\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df['Timestamp_parsed'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "        df['Hour'] = df['Timestamp_parsed'].dt.hour\n",
    "        df['DayOfWeek'] = df['Timestamp_parsed'].dt.dayofweek\n",
    "        df['Month'] = df['Timestamp_parsed'].dt.month\n",
    "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(np.int8)\n",
    "        df['IsRushHour'] = ((df['Hour'] >= 7) & (df['Hour'] <= 9) | \n",
    "                           (df['Hour'] >= 17) & (df['Hour'] <= 19)).astype(np.int8)\n",
    "        df['IsLateNight'] = ((df['Hour'] >= 22) | (df['Hour'] <= 5)).astype(np.int8)\n",
    "        df.drop('Timestamp_parsed', axis=1, inplace=True)\n",
    "    \n",
    "    # Distance and bearing features\n",
    "    coord_cols = ['Pickup_Lat', 'Pickup_Long', 'Dropoff_Lat', 'Dropoff_Long']\n",
    "    if all(col in df.columns for col in coord_cols):\n",
    "        R = 6371\n",
    "        lat1_rad = np.radians(df['Pickup_Lat'])\n",
    "        lat2_rad = np.radians(df['Dropoff_Lat'])\n",
    "        delta_lat = np.radians(df['Dropoff_Lat'] - df['Pickup_Lat'])\n",
    "        delta_lon = np.radians(df['Dropoff_Long'] - df['Pickup_Long'])\n",
    "        a = np.sin(delta_lat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(delta_lon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        df['Haversine_Distance'] = R * c\n",
    "        \n",
    "        df['Bearing'] = compute_bearing(df['Pickup_Lat'], df['Pickup_Long'], \n",
    "                                        df['Dropoff_Lat'], df['Dropoff_Long'])\n",
    "        df['bearing_sin'] = np.sin(np.radians(df['Bearing']))\n",
    "        df['bearing_cos'] = np.cos(np.radians(df['Bearing']))\n",
    "        \n",
    "        df['Delta_Lat'] = df['Dropoff_Lat'] - df['Pickup_Lat']\n",
    "        df['Delta_Long'] = df['Dropoff_Long'] - df['Pickup_Long']\n",
    "        \n",
    "        if 'Distance_KM' in df.columns:\n",
    "            df['Distance_Ratio'] = df['Distance_KM'] / (df['Haversine_Distance'] + 1e-6)\n",
    "            df['Distance_Difference'] = np.abs(df['Distance_KM'] - df['Haversine_Distance'])\n",
    "            df['Is_Ultra_Short'] = (df['Distance_KM'] < 0.024).astype(np.int8)\n",
    "    \n",
    "    # Zone features\n",
    "    if 'Pickup_Zone' in df.columns and 'Dropoff_Zone' in df.columns:\n",
    "        df['Is_Same_Zone'] = (df['Pickup_Zone'] == df['Dropoff_Zone']).astype(np.int8)\n",
    "        \n",
    "        route = df['Pickup_Zone'].astype(str) + '__' + df['Dropoff_Zone'].astype(str)\n",
    "        if is_train:\n",
    "            ROUTE_FREQ = route.value_counts().to_dict()\n",
    "            PICKUP_ZONE_FREQ = df['Pickup_Zone'].value_counts().to_dict()\n",
    "            DROPOFF_ZONE_FREQ = df['Dropoff_Zone'].value_counts().to_dict()\n",
    "        \n",
    "        df['route_count'] = route.map(ROUTE_FREQ).fillna(0).astype(np.int32)\n",
    "        df['pickup_zone_count'] = df['Pickup_Zone'].map(PICKUP_ZONE_FREQ).fillna(0).astype(np.int32)\n",
    "        df['dropoff_zone_count'] = df['Dropoff_Zone'].map(DROPOFF_ZONE_FREQ).fillna(0).astype(np.int32)\n",
    "    \n",
    "    # Sensor features\n",
    "    accel_cols = ['Accel_X', 'Accel_Y', 'Accel_Z']\n",
    "    if all(col in df.columns for col in accel_cols):\n",
    "        df['Accel_Magnitude'] = np.sqrt(df['Accel_X']**2 + df['Accel_Y']**2 + df['Accel_Z']**2)\n",
    "        df['Accel_Max'] = df[accel_cols].max(axis=1)\n",
    "        df['Accel_Min'] = df[accel_cols].min(axis=1)\n",
    "        df['Accel_Range'] = df['Accel_Max'] - df['Accel_Min']\n",
    "        df['Accel_Std'] = df[accel_cols].std(axis=1)\n",
    "    \n",
    "    if 'Gyro_Z' in df.columns:\n",
    "        df['Gyro_Abs'] = np.abs(df['Gyro_Z'])\n",
    "        df['Is_Gyro_Z_Outlier'] = (df['Gyro_Z'].abs() >= 0.672).astype(np.int8)\n",
    "    \n",
    "    # Economic features\n",
    "    if 'Est_Price_IDR' in df.columns and 'Distance_KM' in df.columns:\n",
    "        df['Price_per_KM'] = df['Est_Price_IDR'] / (df['Distance_KM'] + 1e-6)\n",
    "    \n",
    "    if 'Surge_Multiplier' in df.columns:\n",
    "        df['Surge_Category'] = pd.cut(\n",
    "            df['Surge_Multiplier'].fillna(1.0),\n",
    "            bins=[0, 1, 1.5, 2, 10],\n",
    "            labels=[0, 1, 2, 3]\n",
    "        ).astype(np.int8)\n",
    "    \n",
    "    print(f\"Feature engineering completed. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "train = engineer_features(train, is_train=True)\n",
    "test = engineer_features(test, is_train=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6134c0",
   "metadata": {},
   "source": [
    "## 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(train, test, categorical_cols, y_train, smoothing=10):\n",
    "    global_mean = y_train.mean()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        temp_df = pd.DataFrame({col: train[col], 'target': y_train})\n",
    "        agg = temp_df.groupby(col)['target'].agg(['mean', 'count'])\n",
    "        smoothed_mean = (agg['mean'] * agg['count'] + global_mean * smoothing) / (agg['count'] + smoothing)\n",
    "        encoding_map = smoothed_mean.to_dict()\n",
    "        \n",
    "        train[col] = train[col].map(encoding_map).fillna(global_mean).astype(np.float32)\n",
    "        test[col] = test[col].map(encoding_map).fillna(global_mean).astype(np.float32)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def preprocess_data(train, test):\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    cols_to_drop = [config.ID_COL, 'Timestamp']\n",
    "    y = train[config.TARGET_COL].copy()\n",
    "    cols_to_drop.append(config.TARGET_COL)\n",
    "    \n",
    "    cols_to_drop = [col for col in cols_to_drop if col in train.columns]\n",
    "    X_train = train.drop(cols_to_drop, axis=1).copy()\n",
    "    X_test = test.drop([col for col in cols_to_drop if col in test.columns], axis=1).copy()\n",
    "    \n",
    "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Imputation\n",
    "    mean_impute_cols = ['Accel_Y', 'Accel_Z', 'Dropoff_Lat', 'Dropoff_Long', 'Gyro_Z']\n",
    "    mean_impute_cols = [c for c in mean_impute_cols if c in numeric_cols]\n",
    "    \n",
    "    for col in mean_impute_cols:\n",
    "        val = X_train[col].mean()\n",
    "        X_train[col].fillna(val, inplace=True)\n",
    "        X_test[col].fillna(val, inplace=True)\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if X_train[col].isnull().sum() > 0:\n",
    "            val = X_train[col].median()\n",
    "            X_train[col].fillna(val, inplace=True)\n",
    "            X_test[col].fillna(val, inplace=True)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if X_train[col].isnull().sum() > 0:\n",
    "            X_train[col].fillna('Unknown', inplace=True)\n",
    "            X_test[col].fillna('Unknown', inplace=True)\n",
    "    \n",
    "    # Outlier clipping\n",
    "    for col in numeric_cols:\n",
    "        q01, q99 = X_train[col].quantile([0.01, 0.99])\n",
    "        X_train[col] = X_train[col].clip(q01, q99)\n",
    "        X_test[col] = X_test[col].clip(q01, q99)\n",
    "    \n",
    "    # Ordinal encoding\n",
    "    ordinal_cols = ['Weather', 'Traffic', 'Payment_Method', 'Signal_Strength']\n",
    "    ordinal_cols = [c for c in ordinal_cols if c in categorical_cols]\n",
    "    \n",
    "    if ordinal_cols:\n",
    "        oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        X_train[ordinal_cols] = oe.fit_transform(X_train[ordinal_cols])\n",
    "        X_test[ordinal_cols] = oe.transform(X_test[ordinal_cols])\n",
    "        categorical_cols = [c for c in categorical_cols if c not in ordinal_cols]\n",
    "    \n",
    "    # Target encoding\n",
    "    if categorical_cols:\n",
    "        le_temp = LabelEncoder()\n",
    "        y_temp = le_temp.fit_transform(y)\n",
    "        X_train, X_test = encode_target(X_train, X_test, categorical_cols, y_temp, smoothing=10)\n",
    "    \n",
    "    # Target label encoding\n",
    "    le_target = LabelEncoder()\n",
    "    y_encoded = le_target.fit_transform(y)\n",
    "    \n",
    "    print(f\"Final shapes: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
    "    return X_train, X_test, y_encoded, le_target\n",
    "\n",
    "X_train, X_test, y_train, le_target = preprocess_data(train, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348664aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "sample_weights = np.ones(len(y_train), dtype=np.float32)\n",
    "sample_weights[y_train == 1] = config.CLASS_WEIGHTS[1]\n",
    "sample_weights[y_train == 4] = config.CLASS_WEIGHTS[4]\n",
    "\n",
    "print(f\"Class weights applied:\")\n",
    "for cls, weight in config.CLASS_WEIGHTS.items():\n",
    "    print(f\"  Class {cls}: {weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove adversarial features\n",
    "features_to_keep = [c for c in X_train.columns if c not in config.ADVERSARIAL_FEATURES]\n",
    "X_train_clean = X_train[features_to_keep].copy()\n",
    "X_test_clean = X_test[features_to_keep].copy()\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Clean features: {X_train_clean.shape[1]}\")\n",
    "print(f\"Removed: {X_train.shape[1] - X_train_clean.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e7c91",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds_reshaped = preds.reshape(len(labels), -1)\n",
    "    pred_labels = np.argmax(preds_reshaped, axis=1)\n",
    "    score = f1_score(labels, pred_labels, average='macro')\n",
    "    return 'macro_f1', score\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_test, sample_weights):\n",
    "    print(\"Training XGBoost...\")\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'verbosity': 1,\n",
    "        'num_class': len(np.unique(y_train)),\n",
    "        'random_state': config.RANDOM_STATE\n",
    "    }\n",
    "    params.update(config.XGBOOST_PARAMS)\n",
    "    \n",
    "    if config.USE_GPU:\n",
    "        params['device'] = 'cuda'\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.RANDOM_STATE)\n",
    "    \n",
    "    oof_predictions = np.zeros((len(X_train), len(np.unique(y_train))))\n",
    "    test_predictions = np.zeros((len(X_test), len(np.unique(y_train))))\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X_train, y_train), total=config.N_FOLDS, desc=\"XGBoost Folds\"), 1):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_tr, label=y_tr, weight=sample_weights[train_idx])\n",
    "        dval = xgb.DMatrix(X_val, label=y_val, weight=sample_weights[val_idx])\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dtrain, 'train'), (dval, 'valid')],\n",
    "            custom_metric=macro_f1_eval,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=100\n",
    "        )\n",
    "        \n",
    "        oof_predictions[val_idx] = model.predict(dval)\n",
    "        test_predictions += model.predict(dtest) / config.N_FOLDS\n",
    "        \n",
    "        fold_score = f1_score(y_val, np.argmax(oof_predictions[val_idx], axis=1), average='macro')\n",
    "        fold_scores.append(fold_score)\n",
    "        print(f\"Fold {fold}: Macro F1 = {fold_score:.6f}\")\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    cv_score = f1_score(y_train, np.argmax(oof_predictions, axis=1), average='macro')\n",
    "    print(f\"\\nXGBoost CV Score: {cv_score:.6f}\")\n",
    "    \n",
    "    return test_predictions, cv_score, oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_pred, xgb_cv_score, xgb_oof = train_xgboost(\n",
    "    X_train_clean, y_train, X_test_clean, sample_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae16d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost(X_train, y_train, X_test):\n",
    "    print(\"Training CatBoost...\")\n",
    "    \n",
    "    cb_params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.05,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'loss_function': 'MultiClass',\n",
    "        'eval_metric': 'TotalF1:average=Macro',\n",
    "        'random_seed': config.RANDOM_STATE,\n",
    "        'verbose': 100,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'class_weights': config.CLASS_WEIGHTS\n",
    "    }\n",
    "    \n",
    "    if config.USE_GPU:\n",
    "        cb_params['task_type'] = 'GPU'\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.RANDOM_STATE)\n",
    "    \n",
    "    oof_predictions = np.zeros((len(X_train), 5))\n",
    "    test_predictions = np.zeros((len(X_test), 5))\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X_train, y_train), total=config.N_FOLDS, desc=\"CatBoost Folds\"), 1):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        train_pool = Pool(X_tr, y_tr)\n",
    "        val_pool = Pool(X_val, y_val)\n",
    "        \n",
    "        model = CatBoostClassifier(**cb_params)\n",
    "        model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "        \n",
    "        oof_predictions[val_idx] = model.predict_proba(X_val)\n",
    "        test_predictions += model.predict_proba(X_test) / config.N_FOLDS\n",
    "        \n",
    "        fold_score = f1_score(y_val, np.argmax(oof_predictions[val_idx], axis=1), average='macro')\n",
    "        fold_scores.append(fold_score)\n",
    "        print(f\"Fold {fold}: Macro F1 = {fold_score:.6f}\")\n",
    "        \n",
    "        del model, train_pool, val_pool\n",
    "        gc.collect()\n",
    "    \n",
    "    cv_score = f1_score(y_train, np.argmax(oof_predictions, axis=1), average='macro')\n",
    "    print(f\"\\nCatBoost CV Score: {cv_score:.6f}\")\n",
    "    \n",
    "    return test_predictions, cv_score, oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79863cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_test_pred, cb_cv_score, cb_oof = train_catboost(\n",
    "    X_train_clean, y_train, X_test_clean\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a3d1b",
   "metadata": {},
   "source": [
    "## 6. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finding optimal ensemble weights...\")\n",
    "print(f\"\\nIndividual Model CV Scores:\")\n",
    "print(f\"  XGBoost:  {xgb_cv_score:.6f}\")\n",
    "print(f\"  CatBoost: {cb_cv_score:.6f}\")\n",
    "\n",
    "best_ensemble_score = 0\n",
    "best_weights = None\n",
    "\n",
    "for w_xgb in np.arange(0.3, 0.8, 0.05):\n",
    "    w_cb = 1.0 - w_xgb\n",
    "    ensemble_oof = w_xgb * xgb_oof + w_cb * cb_oof\n",
    "    ensemble_pred = np.argmax(ensemble_oof, axis=1)\n",
    "    ensemble_score = f1_score(y_train, ensemble_pred, average='macro')\n",
    "    \n",
    "    if ensemble_score > best_ensemble_score:\n",
    "        best_ensemble_score = ensemble_score\n",
    "        best_weights = (w_xgb, w_cb)\n",
    "        print(f\"  XGB={w_xgb:.2f}, CB={w_cb:.2f} -> F1={ensemble_score:.6f}\")\n",
    "\n",
    "print(f\"\\nBest Ensemble Weights: XGB={best_weights[0]:.2f}, CB={best_weights[1]:.2f}\")\n",
    "print(f\"Best Ensemble CV Score: {best_ensemble_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67333fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xgb, w_cb = best_weights\n",
    "\n",
    "# OOF ensemble with thresholds\n",
    "ensemble_oof = w_xgb * xgb_oof + w_cb * cb_oof\n",
    "ensemble_oof_thresh = ensemble_oof * config.THRESHOLDS\n",
    "ensemble_pred_thresh = np.argmax(ensemble_oof_thresh, axis=1)\n",
    "ensemble_score_thresh = f1_score(y_train, ensemble_pred_thresh, average='macro')\n",
    "\n",
    "print(f\"Ensemble + Thresholds CV Score: {ensemble_score_thresh:.6f}\")\n",
    "print(f\"Improvement from thresholds: {ensemble_score_thresh - best_ensemble_score:+.6f}\")\n",
    "\n",
    "# Test set ensemble with thresholds\n",
    "ensemble_test = w_xgb * xgb_test_pred + w_cb * cb_test_pred\n",
    "ensemble_test_thresh = ensemble_test * config.THRESHOLDS\n",
    "ensemble_test_pred_labels = np.argmax(ensemble_test_thresh, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59a667",
   "metadata": {},
   "source": [
    "## 7. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(test_ids, predictions, le_target, filename):\n",
    "    pred_labels = le_target.inverse_transform(predictions)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        config.ID_COL: test_ids,\n",
    "        config.TARGET_COL: pred_labels\n",
    "    })\n",
    "    \n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Submission saved to: {filename}\")\n",
    "    print(f\"Shape: {submission.shape}\")\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    print(submission[config.TARGET_COL].value_counts())\n",
    "    \n",
    "    return submission\n",
    "\n",
    "submission = create_submission(\n",
    "    test_ids,\n",
    "    ensemble_test_pred_labels,\n",
    "    le_target,\n",
    "    'submission-ensemble.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c0dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison\n",
    "print(\"Prediction Distribution vs Training Distribution:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "pred_dist = submission[config.TARGET_COL].value_counts(normalize=True).sort_index()\n",
    "\n",
    "for label in train_distribution.index:\n",
    "    pred_pct = pred_dist.get(label, 0) * 100\n",
    "    train_pct = train_distribution.get(label, 0) * 100\n",
    "    diff = pred_pct - train_pct\n",
    "    print(f\"  {label}: {pred_pct:.2f}% (train: {train_pct:.2f}%, diff: {diff:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033ec12",
   "metadata": {},
   "source": [
    "## 8. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175aae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Checks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "assert submission.shape[0] == len(test_ids), \"Submission size mismatch\"\n",
    "assert submission.columns.tolist() == [config.ID_COL, config.TARGET_COL], \"Column names mismatch\"\n",
    "assert submission[config.TARGET_COL].isnull().sum() == 0, \"Null predictions found\"\n",
    "\n",
    "expected_labels = set(le_target.classes_)\n",
    "submission_labels = set(submission[config.TARGET_COL].unique())\n",
    "assert submission_labels.issubset(expected_labels), \"Invalid labels in submission\"\n",
    "\n",
    "print(\"All validation checks passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original submission\n",
    "print(\"Comparing with original submission...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    original_submission = pd.read_csv('/kaggle/working/submission-12.csv')\n",
    "    \n",
    "    # Check if both have same shape\n",
    "    print(f\"Original shape: {original_submission.shape}\")\n",
    "    print(f\"New shape: {submission.shape}\")\n",
    "    \n",
    "    # Merge and compare\n",
    "    comparison = submission.merge(\n",
    "        original_submission, \n",
    "        on=config.ID_COL, \n",
    "        suffixes=('_new', '_original')\n",
    "    )\n",
    "    \n",
    "    matches = (comparison[f'{config.TARGET_COL}_new'] == comparison[f'{config.TARGET_COL}_original']).sum()\n",
    "    total = len(comparison)\n",
    "    match_pct = matches / total * 100\n",
    "    \n",
    "    print(f\"\\nMatching predictions: {matches:,} / {total:,} ({match_pct:.2f}%)\")\n",
    "    \n",
    "    if matches == total:\n",
    "        print(\"\\nSUCCESS: New pipeline produces identical results.\")\n",
    "    else:\n",
    "        print(f\"\\nDifferences found: {total - matches:,} predictions differ.\")\n",
    "        \n",
    "        # Show distribution of differences\n",
    "        diff_mask = comparison[f'{config.TARGET_COL}_new'] != comparison[f'{config.TARGET_COL}_original']\n",
    "        if diff_mask.sum() > 0:\n",
    "            print(\"\\nDifference breakdown:\")\n",
    "            diff_df = comparison[diff_mask]\n",
    "            print(diff_df[[f'{config.TARGET_COL}_new', f'{config.TARGET_COL}_original']].value_counts().head(10))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Original submission file not found at /kaggle/working/submission-12.csv\")\n",
    "    print(\"Skipping comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad4c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPipeline Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"XGBoost CV Score:           {xgb_cv_score:.6f}\")\n",
    "print(f\"CatBoost CV Score:          {cb_cv_score:.6f}\")\n",
    "print(f\"Ensemble CV Score:          {best_ensemble_score:.6f}\")\n",
    "print(f\"Ensemble + Thresholds CV:   {ensemble_score_thresh:.6f}\")\n",
    "print(f\"\\nEnsemble Weights: XGB={best_weights[0]:.2f}, CB={best_weights[1]:.2f}\")\n",
    "print(f\"Thresholds: {config.THRESHOLDS}\")\n",
    "print(f\"\\nSubmission saved to: submission-ensemble.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
