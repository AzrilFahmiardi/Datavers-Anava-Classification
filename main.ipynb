{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14452162,"sourceType":"datasetVersion","datasetId":9230974}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2b4cb422","cell_type":"markdown","source":"## 1. Configuration & Setup","metadata":{}},{"id":"0ba4bde3","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom datetime import datetime\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\nimport gc\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\n\nprint(\"Libraries imported successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T13:48:05.834549Z","iopub.execute_input":"2026-01-11T13:48:05.835486Z","iopub.status.idle":"2026-01-11T13:48:06.264815Z","shell.execute_reply.started":"2026-01-11T13:48:05.835448Z","shell.execute_reply":"2026-01-11T13:48:06.264184Z"}},"outputs":[{"name":"stdout","text":"Libraries imported successfully.\n","output_type":"stream"}],"execution_count":2},{"id":"9d95450b","cell_type":"code","source":"USE_GPU = True  \n\ngpu_config = {\n    'catboost_gpu': USE_GPU,\n    'lightgbm_gpu': USE_GPU,\n    'xgboost_gpu': USE_GPU\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T14:08:37.803444Z","iopub.execute_input":"2026-01-11T14:08:37.803765Z","iopub.status.idle":"2026-01-11T14:08:37.807734Z","shell.execute_reply.started":"2026-01-11T14:08:37.803739Z","shell.execute_reply":"2026-01-11T14:08:37.806951Z"}},"outputs":[],"execution_count":14},{"id":"41cd27c8","cell_type":"code","source":"class Config:\n    DATA_PATH = '/kaggle/input/ride-hailing-trip-classification-dataset/'\n    \n    N_FOLDS = 5\n    RANDOM_STATE = 42\n    TARGET_COL = 'Trip_Label'\n    ID_COL = 'Trip_ID'\n    \n    USE_TEMPORAL = False\n    USE_DISTANCE = True\n    USE_SENSOR_AGG = True\n    USE_ECONOMIC = False\n    USE_INTERACTION = True\n    \n    @staticmethod\n    def get_catboost_params(use_gpu=False):\n        params = {\n            'iterations': 1000,\n            'learning_rate': 0.05,\n            'depth': 6,\n            'loss_function': 'MultiClass',\n            'eval_metric': 'TotalF1:average=Macro',\n            'auto_class_weights': 'Balanced',\n            'random_seed': 42,\n            'verbose': 100,\n            'early_stopping_rounds': 50\n        }\n        if use_gpu:\n            params['task_type'] = 'GPU'\n            params['devices'] = '0'\n            print(\"  CatBoost: GPU mode activated\")\n        else:\n            params['task_type'] = 'CPU'\n            print(\"  CatBoost: CPU mode\")\n        return params\n    \n    @staticmethod\n    def get_lightgbm_params(use_gpu=False):\n        params = {\n            'objective': 'multiclass',\n            'metric': 'multi_logloss',\n            'boosting_type': 'gbdt',\n            'num_leaves': 31,\n            'learning_rate': 0.05,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'random_state': 42,\n            'verbose': -1,\n            'min_data_in_leaf': 100,\n            'min_sum_hessian_in_leaf': 1e-2\n        }\n        if use_gpu:\n            params['device'] = 'gpu'\n            print(\"  LightGBM: GPU mode activated\")\n        else:\n            params['device'] = 'cpu'\n            print(\"  LightGBM: CPU mode\")\n        return params\n    \n    @staticmethod\n    def get_xgboost_params(use_gpu=False):\n        params = {\n            'objective': 'multi:softprob',\n            'eval_metric': 'mlogloss',\n            'max_depth': 6,\n            'learning_rate': 0.05,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'random_state': 42,\n            'verbosity': 1\n        }\n        if use_gpu:\n            params['device'] = 'cuda'\n            print(\"  XGBoost: GPU mode activated\")\n        else:\n            params['device'] = 'cpu'\n            print(\"  XGBoost: CPU mode\")\n        return params\n\nconfig = Config()\nprint(\"\\nConfiguration loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-11T13:48:11.682962Z","iopub.execute_input":"2026-01-11T13:48:11.683538Z","iopub.status.idle":"2026-01-11T13:48:11.692343Z","shell.execute_reply.started":"2026-01-11T13:48:11.683502Z","shell.execute_reply":"2026-01-11T13:48:11.691648Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nConfiguration loaded successfully.\n","output_type":"stream"}],"execution_count":3},{"id":"3134a05c","cell_type":"markdown","source":"## 2. Data Loading & Validation","metadata":{}},{"id":"37144dad","cell_type":"code","source":"def load_data():\n    print(\"Loading data...\")\n    files = ['train.csv', 'test.csv', 'sample_submission.csv']\n    data = {}\n    \n    for file in tqdm(files, desc=\"Loading files\"):\n        data[file.replace('.csv', '')] = pd.read_csv(config.DATA_PATH + file)\n    \n    train = data['train']\n    test = data['test']\n    sample_submission = data['sample_submission']\n    \n    print(f\"Train shape: {train.shape}\")\n    print(f\"Test shape: {test.shape}\")\n    print(f\"Sample submission shape: {sample_submission.shape}\")\n    \n    if config.TARGET_COL in train.columns:\n        print(f\"\\nTarget distribution:\")\n        print(train[config.TARGET_COL].value_counts())\n    \n    return train, test, sample_submission\n\ntrain, test, sample_submission = load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T13:48:13.876145Z","iopub.execute_input":"2026-01-11T13:48:13.876802Z","iopub.status.idle":"2026-01-11T13:49:39.828997Z","shell.execute_reply.started":"2026-01-11T13:48:13.876773Z","shell.execute_reply":"2026-01-11T13:49:39.828145Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd03be23aef485d94187e45a129d65a"}},"metadata":{}},{"name":"stdout","text":"Train shape: (8000000, 25)\nTest shape: (4000000, 24)\nSample submission shape: (4000000, 2)\n\nTarget distribution:\nTrip_Label\nPerfect_Trip         4397607\nSafety_Violation     1601595\nNavigation_Issue      801790\nService_Complaint     798695\nFraud_Indication      400313\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"id":"74d1d42f","cell_type":"code","source":"def optimize_memory(df):\n    print(f\"Optimizing memory for dataframe with {len(df.columns)} columns...\")\n    for col in tqdm(df.columns, desc=\"Optimizing columns\"):\n        col_type = df[col].dtype\n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n    return df\n\nprint(\"Optimizing memory...\")\ntrain = optimize_memory(train)\ntest = optimize_memory(test)\nprint(\"Memory optimization completed.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-11T13:49:39.830447Z","iopub.execute_input":"2026-01-11T13:49:39.830749Z","iopub.status.idle":"2026-01-11T13:49:40.539673Z","shell.execute_reply.started":"2026-01-11T13:49:39.830725Z","shell.execute_reply":"2026-01-11T13:49:40.538879Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Optimizing memory...\nOptimizing memory for dataframe with 25 columns...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Optimizing columns:   0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10d1d677a2744d12b10b323194f0bf73"}},"metadata":{}},{"name":"stdout","text":"Optimizing memory for dataframe with 24 columns...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Optimizing columns:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86b2a2869bf49078d1774f4158c059d"}},"metadata":{}},{"name":"stdout","text":"Memory optimization completed.\n","output_type":"stream"}],"execution_count":5},{"id":"2b7bf178","cell_type":"markdown","source":"## 3. Feature Engineering","metadata":{}},{"id":"bb072add","cell_type":"code","source":"def haversine_distance(lat1, lon1, lat2, lon2):\n    R = 6371\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    delta_lat = np.radians(lat2 - lat1)\n    delta_lon = np.radians(lon2 - lon1)\n    \n    a = np.sin(delta_lat/2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(delta_lon/2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    \n    return R * c\n\ndef engineer_features(df, is_train=True):\n    print(f\"Engineering features for {'train' if is_train else 'test'} set...\")\n    df = df.copy()\n    \n    if config.USE_TEMPORAL and 'Timestamp' in df.columns:\n        print(\"  - Creating temporal features...\")\n        df['Timestamp_parsed'] = pd.to_datetime(df['Timestamp'])\n        df['Hour'] = df['Timestamp_parsed'].dt.hour.astype(np.int8)\n        df['DayOfWeek'] = df['Timestamp_parsed'].dt.dayofweek.astype(np.int8)\n        df['Month'] = df['Timestamp_parsed'].dt.month.astype(np.int8)\n        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(np.int8)\n        df['IsRushHour'] = ((df['Hour'] >= 7) & (df['Hour'] <= 9) | \n                            (df['Hour'] >= 17) & (df['Hour'] <= 19)).astype(np.int8)\n        df['IsLateNight'] = ((df['Hour'] >= 22) | (df['Hour'] <= 5)).astype(np.int8)\n        df.drop('Timestamp_parsed', axis=1, inplace=True)\n    \n    if config.USE_DISTANCE:\n        print(\"  - Creating distance features...\")\n        if all(col in df.columns for col in ['Pickup_Lat', 'Pickup_Long', 'Dropoff_Lat', 'Dropoff_Long']):\n            df['Haversine_Distance'] = haversine_distance(\n                df['Pickup_Lat'], df['Pickup_Long'],\n                df['Dropoff_Lat'], df['Dropoff_Long']\n            )\n            \n            if 'Distance_KM' in df.columns:\n                df['Distance_Ratio'] = df['Distance_KM'] / (df['Haversine_Distance'] + 1e-6)\n                df['Distance_Difference'] = np.abs(df['Distance_KM'] - df['Haversine_Distance'])\n        \n        if 'Pickup_Zone' in df.columns and 'Dropoff_Zone' in df.columns:\n            df['Is_Same_Zone'] = (df['Pickup_Zone'] == df['Dropoff_Zone']).astype(np.int8)\n    \n    if config.USE_SENSOR_AGG:\n        print(\"  - Creating sensor aggregation features...\")\n        if all(col in df.columns for col in ['Accel_X', 'Accel_Y', 'Accel_Z']):\n            df['Accel_Magnitude'] = np.sqrt(df['Accel_X']**2 + df['Accel_Y']**2 + df['Accel_Z']**2)\n            df['Accel_Max'] = df[['Accel_X', 'Accel_Y', 'Accel_Z']].max(axis=1)\n            df['Accel_Min'] = df[['Accel_X', 'Accel_Y', 'Accel_Z']].min(axis=1)\n            df['Accel_Std'] = df[['Accel_X', 'Accel_Y', 'Accel_Z']].std(axis=1)\n            df['Accel_Range'] = df['Accel_Max'] - df['Accel_Min']\n        \n        if 'Gyro_Z' in df.columns:\n            df['Gyro_Abs'] = np.abs(df['Gyro_Z'])\n    \n    if config.USE_ECONOMIC:\n        print(\"  - Creating economic features...\")\n        if 'Est_Price_IDR' in df.columns and 'Distance_KM' in df.columns:\n            df['Price_per_KM'] = df['Est_Price_IDR'] / (df['Distance_KM'] + 1e-6)\n        \n        if 'Promo_Code' in df.columns:\n            df['Has_Promo'] = (df['Promo_Code'].notna()).astype(np.int8)\n        \n        if 'Surge_Multiplier' in df.columns:\n            df['Surge_Category'] = pd.cut(df['Surge_Multiplier'], \n                                          bins=[0, 1, 1.5, 2, 10], \n                                          labels=[0, 1, 2, 3]).astype(np.int8)\n    \n    if config.USE_INTERACTION:\n        print(\"  - Creating interaction features...\")\n        if 'Surge_Multiplier' in df.columns and 'Hour' in df.columns:\n            df['Surge_Hour_Interaction'] = df['Surge_Multiplier'] * df['Hour']\n        \n        if 'Distance_KM' in df.columns and 'Traffic' in df.columns:\n            traffic_map = {'Light': 1, 'Moderate': 2, 'Heavy': 3}\n            df['Traffic_Numeric'] = df['Traffic'].map(traffic_map).fillna(0).astype(np.int8)\n            df['Distance_Traffic'] = df['Distance_KM'] * df['Traffic_Numeric']\n    \n    print(f\"Feature engineering completed. Shape: {df.shape}\")\n    return df\n\ntrain = engineer_features(train, is_train=True)\ntest = engineer_features(test, is_train=False)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2026-01-11T13:49:40.540641Z","iopub.execute_input":"2026-01-11T13:49:40.541209Z","iopub.status.idle":"2026-01-11T13:49:49.998715Z","shell.execute_reply.started":"2026-01-11T13:49:40.541188Z","shell.execute_reply":"2026-01-11T13:49:49.998108Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Engineering features for train set...\n  - Creating distance features...\n  - Creating sensor aggregation features...\n  - Creating interaction features...\nFeature engineering completed. Shape: (8000000, 37)\nEngineering features for test set...\n  - Creating distance features...\n  - Creating sensor aggregation features...\n  - Creating interaction features...\nFeature engineering completed. Shape: (4000000, 36)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"27"},"metadata":{}}],"execution_count":6},{"id":"2d23f1e9","cell_type":"markdown","source":"## 4. Preprocessing","metadata":{}},{"id":"e9713440","cell_type":"code","source":"def encode_categorical_frequency(train, test, categorical_cols):\n    \"\"\"\n    Encodes based on value frequency in training data\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"FREQUENCY ENCODING\")\n    print(\"=\"*80)\n    \n    encoders = {}\n    \n    for col in tqdm(categorical_cols, desc=\"Frequency encoding\"):\n        freq_map = train[col].value_counts(dropna=False).to_dict()\n        \n        train[col] = train[col].map(freq_map).fillna(0).astype(np.int32)\n        test[col] = test[col].map(freq_map).fillna(0).astype(np.int32)\n        \n        encoders[col] = {\n            'type': 'frequency',\n            'unique_values': len(freq_map),\n            'unseen_in_test': (test[col] == 0).sum()\n        }\n    \n    print(f\"\\nEncoded {len(categorical_cols)} categorical features\")\n    return train, test, encoders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T13:49:50.000548Z","iopub.execute_input":"2026-01-11T13:49:50.000885Z","iopub.status.idle":"2026-01-11T13:49:50.006759Z","shell.execute_reply.started":"2026-01-11T13:49:50.000863Z","shell.execute_reply":"2026-01-11T13:49:50.006200Z"}},"outputs":[],"execution_count":7},{"id":"b7bb1a79","cell_type":"code","source":"def encode_categorical_target(train, test, categorical_cols, y_train, smoothing=10):\n    \"\"\"\n    Encodes based on target mean, with smoothing to prevent overfitting\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"TARGET ENCODING\")\n    print(\"=\"*80)\n    \n    encoders = {}\n    global_mean = y_train.mean()\n    \n    for col in tqdm(categorical_cols, desc=\"Target encoding\"):\n        temp_df = pd.DataFrame({col: train[col], 'target': y_train})\n        \n        agg = temp_df.groupby(col)['target'].agg(['mean', 'count'])\n        smoothed_mean = (agg['mean'] * agg['count'] + global_mean * smoothing) / (agg['count'] + smoothing)\n        \n        encoding_map = smoothed_mean.to_dict()\n        \n        train[col] = train[col].map(encoding_map).fillna(global_mean).astype(np.float32)\n        test[col] = test[col].map(encoding_map).fillna(global_mean).astype(np.float32)\n        \n        encoders[col] = {\n            'type': 'target',\n            'unique_values': len(encoding_map),\n            'global_mean': global_mean\n        }\n    \n    print(f\"\\nEncoded {len(categorical_cols)} categorical features\")\n    return train, test, encoders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T13:49:50.026206Z","iopub.execute_input":"2026-01-11T13:49:50.026531Z","iopub.status.idle":"2026-01-11T13:49:50.045990Z","shell.execute_reply.started":"2026-01-11T13:49:50.026510Z","shell.execute_reply":"2026-01-11T13:49:50.045396Z"}},"outputs":[],"execution_count":9},{"id":"196940cd","cell_type":"code","source":"def encode_categorical_label_optimized(train, test, categorical_cols):\n    \"\"\"\n    Uses map() for vectorized operations\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"LABEL ENCODING \")\n    print(\"=\"*80)\n    \n    encoders = {}\n    \n    for col in tqdm(categorical_cols, desc=\"Label encoding\"):\n        train[col] = train[col].astype(str)\n        test[col] = test[col].astype(str)\n        \n        le = LabelEncoder()\n        train[col] = le.fit_transform(train[col])\n        \n        mapping = {label: idx for idx, label in enumerate(le.classes_)}\n        test[col] = test[col].map(mapping).fillna(-1).astype(np.int32)\n        \n        encoders[col] = {\n            'type': 'label',\n            'encoder': le,\n            'unique_values': len(le.classes_),\n            'unseen_in_test': (test[col] == -1).sum()\n        }\n    \n    print(f\"\\nEncoded {len(categorical_cols)} categorical features\")\n    return train, test, encoders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T13:49:50.046958Z","iopub.execute_input":"2026-01-11T13:49:50.047217Z","iopub.status.idle":"2026-01-11T13:49:50.068242Z","shell.execute_reply.started":"2026-01-11T13:49:50.047196Z","shell.execute_reply":"2026-01-11T13:49:50.067383Z"}},"outputs":[],"execution_count":10},{"id":"445248f0","cell_type":"code","source":"def preprocess_data(train, test, encoding_method='frequency'):\n    \"\"\"\n    Main preprocessing pipeline\n    \n    Parameters:\n    -----------\n    train : pd.DataFrame\n        Training dataset\n    test : pd.DataFrame\n        Test dataset\n    encoding_method : str\n        Encoding method to use: 'frequency', 'target', or 'label'\n    Returns:\n    --------\n    X_train, X_test, y_train, le_target, encoders\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"DATA PREPROCESSING PIPELINE\")\n    print(\"=\"*80)\n    print(f\"Encoding method: {encoding_method.upper()}\")\n    \n    cols_to_drop = [config.ID_COL, 'Timestamp']\n    if config.TARGET_COL in train.columns:\n        y = train[config.TARGET_COL].copy()\n        cols_to_drop.append(config.TARGET_COL)\n    else:\n        y = None\n    \n    cols_to_drop = [col for col in cols_to_drop if col in train.columns]\n    X_train = train.drop(cols_to_drop, axis=1).copy()\n    X_test = test.drop([col for col in cols_to_drop if col in test.columns], axis=1).copy()\n    \n    print(f\"\\nInitial shapes:\")\n    print(f\"  X_train: {X_train.shape}\")\n    print(f\"  X_test: {X_test.shape}\")\n    \n    print(f\"\\nMissing values before imputation:\")\n    train_missing = X_train.isnull().sum()\n    if train_missing.sum() > 0:\n        print(train_missing[train_missing > 0])\n    else:\n        print(\"  No missing values found\")\n    \n    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n    \n    print(f\"\\nFeature types detected:\")\n    print(f\"  Numeric features: {len(numeric_cols)}\")\n    print(f\"  Categorical features: {len(categorical_cols)}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 1: Missing Value Imputation\")\n    print(\"=\"*80)\n    \n    for col in tqdm(numeric_cols, desc=\"Imputing numeric features\"):\n        if X_train[col].isnull().sum() > 0:\n            median_val = X_train[col].median()\n            X_train[col].fillna(median_val, inplace=True)\n            X_test[col].fillna(median_val, inplace=True)\n    \n    for col in tqdm(categorical_cols, desc=\"Imputing categorical features\"):\n        if X_train[col].isnull().sum() > 0:\n            X_train[col].fillna('Unknown', inplace=True)\n            X_test[col].fillna('Unknown', inplace=True)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 2: Outlier Clipping\")\n    print(\"=\"*80)\n    \n    for col in tqdm(numeric_cols, desc=\"Clipping outliers\"):\n        q99 = X_train[col].quantile(0.99)\n        q01 = X_train[col].quantile(0.01)\n        X_train[col] = X_train[col].clip(q01, q99)\n        X_test[col] = X_test[col].clip(q01, q99)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 3: Categorical Encoding\")\n    print(\"=\"*80)\n    \n    if len(categorical_cols) > 0:\n        if encoding_method == 'frequency':\n            X_train, X_test, encoders = encode_categorical_frequency(\n                X_train, X_test, categorical_cols\n            )\n        elif encoding_method == 'target':\n            if y is None:\n                raise ValueError(\"Target encoding requires target variable\")\n            le_target_temp = LabelEncoder()\n            y_temp = le_target_temp.fit_transform(y)\n            X_train, X_test, encoders = encode_categorical_target(\n                X_train, X_test, categorical_cols, y_temp\n            )\n        elif encoding_method == 'label':\n            X_train, X_test, encoders = encode_categorical_label_optimized(\n                X_train, X_test, categorical_cols\n            )\n        else:\n            raise ValueError(f\"Unknown encoding method: {encoding_method}\")\n    else:\n        encoders = {}\n        print(\"No categorical features to encode\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 4: Target Encoding\")\n    print(\"=\"*80)\n    \n    if y is not None:\n        le_target = LabelEncoder()\n        y_encoded = le_target.fit_transform(y)\n        print(f\"\\nTarget classes encoded:\")\n        for i, label in enumerate(le_target.classes_):\n            count = (y_encoded == i).sum()\n            print(f\"  {i}: {label:20s} - {count:,} samples ({count/len(y_encoded)*100:.2f}%)\")\n    else:\n        y_encoded = None\n        le_target = None\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PREPROCESSING COMPLETED\")\n    print(\"=\"*80)\n    print(f\"Final shapes:\")\n    print(f\"  X_train: {X_train.shape}\")\n    print(f\"  X_test: {X_test.shape}\")\n    if y_encoded is not None:\n        print(f\"  y_train: {y_encoded.shape}\")\n    print(\"=\"*80)\n    \n    return X_train, X_test, y_encoded, le_target, encoders\n\nX_train, X_test, y_train, le_target, encoders = preprocess_data(train, test, encoding_method='frequency')\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2026-01-11T13:49:50.069344Z","iopub.execute_input":"2026-01-11T13:49:50.069889Z","iopub.status.idle":"2026-01-11T13:50:40.842646Z","shell.execute_reply.started":"2026-01-11T13:49:50.069866Z","shell.execute_reply":"2026-01-11T13:50:40.842051Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n================================================================================\nDATA PREPROCESSING PIPELINE\n================================================================================\nEncoding method: FREQUENCY\n\nInitial shapes:\n  X_train: (8000000, 34)\n  X_test: (4000000, 34)\n\nMissing values before imputation:\nPickup_Lat              929348\nPickup_Long             611083\nDropoff_Lat            1914440\nDropoff_Long           1556029\nGPS_Accuracy_M         1504090\nDistance_KM             941317\nEst_Price_IDR          1151401\nSurge_Multiplier        612307\nAccel_X                1608442\nAccel_Y                1081337\nAccel_Z                1840229\nGyro_Z                  701714\nPickup_Zone            1381274\nDropoff_Zone            765445\nDevice_FP              1499497\nPromo_Code             1107810\nCar_Model              1721777\nPayment_Method         1141190\nWeather                 514649\nTraffic                1808403\nBattery_Level          1746399\nSignal_Strength         408588\nHaversine_Distance     3998467\nDistance_Ratio         4469127\nDistance_Difference    4469127\nAccel_Magnitude        3744751\nAccel_Max                50209\nAccel_Min                50209\nAccel_Std               735048\nAccel_Range              50209\nGyro_Abs                701714\nDistance_Traffic        941317\ndtype: int64\n\nFeature types detected:\n  Numeric features: 24\n  Categorical features: 10\n\n================================================================================\nSTEP 1: Missing Value Imputation\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Imputing numeric features:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e21704511e4b7ea6f914a211e243e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Imputing categorical features:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db983c00a714688af62ef655e182ef6"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nSTEP 2: Outlier Clipping\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Clipping outliers:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7676ddfb465143588f819d737a262863"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nSTEP 3: Categorical Encoding\n================================================================================\n\n================================================================================\nFREQUENCY ENCODING\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Frequency encoding:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"277ad14ab29a4a87a510e5b4106b35dd"}},"metadata":{}},{"name":"stdout","text":"\nEncoded 10 categorical features\n\n================================================================================\nSTEP 4: Target Encoding\n================================================================================\n\nTarget classes encoded:\n  0: Fraud_Indication     - 400,313 samples (5.00%)\n  1: Navigation_Issue     - 801,790 samples (10.02%)\n  2: Perfect_Trip         - 4,397,607 samples (54.97%)\n  3: Safety_Violation     - 1,601,595 samples (20.02%)\n  4: Service_Complaint    - 798,695 samples (9.98%)\n\n================================================================================\nPREPROCESSING COMPLETED\n================================================================================\nFinal shapes:\n  X_train: (8000000, 34)\n  X_test: (4000000, 34)\n  y_train: (8000000,)\n================================================================================\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"78"},"metadata":{}}],"execution_count":11},{"id":"80430d8d","cell_type":"markdown","source":"## 7. Model Training","metadata":{}},{"id":"6340dc9b","cell_type":"code","source":"try:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\nexcept ImportError:\n    print(\"XGBoost not available. Install with: pip install xgboost\")\n    XGBOOST_AVAILABLE = False\n\ndef train_xgboost(X_train, y_train, X_test, n_folds=5, use_gpu=False):\n    if not XGBOOST_AVAILABLE:\n        return None, None, None\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"Training XGBoost Models\")\n    print(\"=\"*80)\n    \n    params = config.get_xgboost_params(use_gpu=use_gpu)\n    params['num_class'] = len(np.unique(y_train))\n    \n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=config.RANDOM_STATE)\n    \n    oof_predictions = np.zeros((len(X_train), len(np.unique(y_train))))\n    test_predictions = np.zeros((len(X_test), len(np.unique(y_train))))\n    \n    fold_scores = []\n    models = []\n    \n    pbar = tqdm(enumerate(skf.split(X_train, y_train), 1), total=n_folds, desc=\"XGBoost Folds\")\n    for fold, (train_idx, val_idx) in pbar:\n        pbar.set_description(f\"XGBoost Fold {fold}/{n_folds}\")\n        \n        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n        \n        dtrain = xgb.DMatrix(X_tr, label=y_tr)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        dtest = xgb.DMatrix(X_test)\n        \n        model = xgb.train(\n            params,\n            dtrain,\n            num_boost_round=1000,\n            evals=[(dtrain, 'train'), (dval, 'valid')],\n            early_stopping_rounds=50,\n            verbose_eval=100\n        )\n        \n        oof_predictions[val_idx] = model.predict(dval)\n        test_predictions += model.predict(dtest) / n_folds\n        \n        oof_pred_labels = np.argmax(oof_predictions[val_idx], axis=1)\n        fold_score = f1_score(y_val, oof_pred_labels, average='macro')\n        fold_scores.append(fold_score)\n        \n        pbar.set_postfix({'F1': f'{fold_score:.6f}'})\n        \n        models.append(model)\n        gc.collect()\n    \n    oof_pred_labels = np.argmax(oof_predictions, axis=1)\n    overall_score = f1_score(y_train, oof_pred_labels, average='macro')\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"XGBoost Overall CV Score: {overall_score:.6f} (+/- {np.std(fold_scores):.6f})\")\n    print(\"=\"*80)\n    \n    return test_predictions, models, overall_score\n\nif XGBOOST_AVAILABLE:\n    xgboost_test_pred, xgboost_models, xgboost_cv_score = train_xgboost(\n        X_train, y_train, X_test,\n        n_folds=config.N_FOLDS,\n        use_gpu=gpu_config['xgboost_gpu']\n    )\nelse:\n    xgboost_test_pred, xgboost_models, xgboost_cv_score = None, None, 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T11:48:18.582917Z","iopub.execute_input":"2026-01-11T11:48:18.583209Z","iopub.status.idle":"2026-01-11T11:59:38.399830Z","shell.execute_reply.started":"2026-01-11T11:48:18.583186Z","shell.execute_reply":"2026-01-11T11:59:38.399109Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTraining XGBoost Models\n================================================================================\n  XGBoost: GPU mode activated\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"XGBoost Folds:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe7eaacaa0fa45b587a39c6c362dbd06"}},"metadata":{}},{"name":"stdout","text":"[0]\ttrain-mlogloss:1.40560\tvalid-mlogloss:1.40563\n[100]\ttrain-mlogloss:0.63047\tvalid-mlogloss:0.63111\n[200]\ttrain-mlogloss:0.61240\tvalid-mlogloss:0.61311\n[300]\ttrain-mlogloss:0.61031\tvalid-mlogloss:0.61148\n[400]\ttrain-mlogloss:0.60946\tvalid-mlogloss:0.61122\n[500]\ttrain-mlogloss:0.60860\tvalid-mlogloss:0.61108\n[600]\ttrain-mlogloss:0.60778\tvalid-mlogloss:0.61104\n[700]\ttrain-mlogloss:0.60698\tvalid-mlogloss:0.61102\n[727]\ttrain-mlogloss:0.60677\tvalid-mlogloss:0.61101\n[0]\ttrain-mlogloss:1.40559\tvalid-mlogloss:1.40559\n[100]\ttrain-mlogloss:0.63068\tvalid-mlogloss:0.63058\n[200]\ttrain-mlogloss:0.61259\tvalid-mlogloss:0.61250\n[300]\ttrain-mlogloss:0.61050\tvalid-mlogloss:0.61088\n[400]\ttrain-mlogloss:0.60964\tvalid-mlogloss:0.61063\n[500]\ttrain-mlogloss:0.60877\tvalid-mlogloss:0.61049\n[600]\ttrain-mlogloss:0.60792\tvalid-mlogloss:0.61042\n[700]\ttrain-mlogloss:0.60712\tvalid-mlogloss:0.61041\n[731]\ttrain-mlogloss:0.60688\tvalid-mlogloss:0.61041\n[0]\ttrain-mlogloss:1.40560\tvalid-mlogloss:1.40560\n[100]\ttrain-mlogloss:0.63051\tvalid-mlogloss:0.63084\n[200]\ttrain-mlogloss:0.61243\tvalid-mlogloss:0.61286\n[300]\ttrain-mlogloss:0.61035\tvalid-mlogloss:0.61127\n[400]\ttrain-mlogloss:0.60951\tvalid-mlogloss:0.61105\n[500]\ttrain-mlogloss:0.60865\tvalid-mlogloss:0.61093\n[600]\ttrain-mlogloss:0.60780\tvalid-mlogloss:0.61087\n[700]\ttrain-mlogloss:0.60698\tvalid-mlogloss:0.61085\n[800]\ttrain-mlogloss:0.60620\tvalid-mlogloss:0.61084\n[825]\ttrain-mlogloss:0.60601\tvalid-mlogloss:0.61085\n[0]\ttrain-mlogloss:1.40562\tvalid-mlogloss:1.40559\n[100]\ttrain-mlogloss:0.63068\tvalid-mlogloss:0.63017\n[200]\ttrain-mlogloss:0.61262\tvalid-mlogloss:0.61214\n[300]\ttrain-mlogloss:0.61054\tvalid-mlogloss:0.61054\n[400]\ttrain-mlogloss:0.60969\tvalid-mlogloss:0.61030\n[500]\ttrain-mlogloss:0.60881\tvalid-mlogloss:0.61017\n[600]\ttrain-mlogloss:0.60797\tvalid-mlogloss:0.61011\n[700]\ttrain-mlogloss:0.60716\tvalid-mlogloss:0.61010\n[800]\ttrain-mlogloss:0.60637\tvalid-mlogloss:0.61008\n[900]\ttrain-mlogloss:0.60560\tvalid-mlogloss:0.61009\n[912]\ttrain-mlogloss:0.60551\tvalid-mlogloss:0.61009\n[0]\ttrain-mlogloss:1.40559\tvalid-mlogloss:1.40559\n[100]\ttrain-mlogloss:0.63060\tvalid-mlogloss:0.63074\n[200]\ttrain-mlogloss:0.61249\tvalid-mlogloss:0.61274\n[300]\ttrain-mlogloss:0.61038\tvalid-mlogloss:0.61113\n[400]\ttrain-mlogloss:0.60953\tvalid-mlogloss:0.61088\n[500]\ttrain-mlogloss:0.60866\tvalid-mlogloss:0.61075\n[600]\ttrain-mlogloss:0.60783\tvalid-mlogloss:0.61070\n[700]\ttrain-mlogloss:0.60703\tvalid-mlogloss:0.61068\n[760]\ttrain-mlogloss:0.60655\tvalid-mlogloss:0.61068\n\n================================================================================\nXGBoost Overall CV Score: 0.560081 (+/- 0.000121)\n================================================================================\n","output_type":"stream"}],"execution_count":24},{"id":"69abb317","cell_type":"markdown","source":"## 8. Model Selection & Prediction Strategy","metadata":{}},{"id":"a8a333a8","cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"MODEL COMPARISON & SELECTION\")\nprint(\"=\"*80)\n\nmodel_results = []\n\nif catboost_test_pred is not None:\n    model_results.append({\n        'name': 'CatBoost',\n        'cv_score': catboost_cv_score,\n        'predictions': catboost_test_pred\n    })\n\nif lightgbm_test_pred is not None:\n    model_results.append({\n        'name': 'LightGBM',\n        'cv_score': lightgbm_cv_score,\n        'predictions': lightgbm_test_pred\n    })\n\nif xgboost_test_pred is not None:\n    model_results.append({\n        'name': 'XGBoost',\n        'cv_score': xgboost_cv_score,\n        'predictions': xgboost_test_pred\n    })\n\nif len(model_results) == 0:\n    raise ValueError(\"No models trained successfully!\")\n\nprint(\"\\nModel Performance Comparison:\")\nprint(\"-\" * 80)\nfor result in sorted(model_results, key=lambda x: x['cv_score'], reverse=True):\n    print(f\"  {result['name']:15s}: CV Score = {result['cv_score']:.6f}\")\n\nbest_model = max(model_results, key=lambda x: x['cv_score'])\nprint(\"\\n\" + \"=\"*80)\nprint(f\"BEST MODEL: {best_model['name']} (CV Score: {best_model['cv_score']:.6f})\")\nprint(\"=\"*80)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PREDICTION STRATEGY SELECTION\")\nprint(\"=\"*80)\n\nUSE_ENSEMBLE = False  \n\nif USE_ENSEMBLE and len(model_results) > 1:\n    print(\"\\n✓ Using ENSEMBLE of all models (equal weights)\")\n    \n    ensemble_pred = np.zeros_like(model_results[0]['predictions'])\n    for result in model_results:\n        ensemble_pred += result['predictions'] / len(model_results)\n    \n    final_predictions = ensemble_pred\n    strategy_name = f\"Ensemble of {len(model_results)} models\"\nelse:\n    print(f\"\\n✓ Using BEST INDIVIDUAL MODEL: {best_model['name']}\")\n    final_predictions = best_model['predictions']\n    strategy_name = best_model['name']\n\nfinal_pred_labels = np.argmax(final_predictions, axis=1)\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"Final Strategy: {strategy_name}\")\nprint(f\"Predictions shape: {final_predictions.shape}\")\nprint(\"=\"*80)","metadata":{},"outputs":[],"execution_count":null},{"id":"f5ebc841","cell_type":"markdown","source":"## 9. Generate Submission","metadata":{}},{"id":"ebd660f5","cell_type":"code","source":"def create_submission(test_ids, predictions, le_target, filename='submission.csv'):\n    pred_labels = le_target.inverse_transform(predictions)\n    \n    submission = pd.DataFrame({\n        config.ID_COL: test_ids,\n        config.TARGET_COL: pred_labels\n    })\n    \n    submission.to_csv(filename, index=False)\n    \n    print(f\"\\nSubmission saved to: {filename}\")\n    print(f\"Submission shape: {submission.shape}\")\n    print(f\"\\nPrediction distribution:\")\n    print(submission[config.TARGET_COL].value_counts())\n    \n    return submission\n\ntest_ids = test[config.ID_COL].values\nsubmission = create_submission(test_ids, final_pred_labels, le_target, 'submission.csv')\nsubmission.head(10)","metadata":{},"outputs":[],"execution_count":null},{"id":"e64c9f05","cell_type":"markdown","source":"## 10. Validation & Analysis","metadata":{}},{"id":"18f7d50d","cell_type":"code","source":"print(\"\\n\" + \"=\"*80)\nprint(\"Final Validation Checks\")\nprint(\"=\"*80)\n\nassert submission.shape[0] == test.shape[0], \"Submission size mismatch!\"\nassert submission.columns.tolist() == [config.ID_COL, config.TARGET_COL], \"Column names mismatch!\"\nassert submission[config.TARGET_COL].isnull().sum() == 0, \"Null predictions found!\"\n\nexpected_labels = set(le_target.classes_)\nsubmission_labels = set(submission[config.TARGET_COL].unique())\nassert submission_labels.issubset(expected_labels), \"Invalid labels in submission!\"\n\nprint(\"All validation checks passed!\")\nprint(\"=\"*80)","metadata":{},"outputs":[],"execution_count":null},{"id":"a0bc8087","cell_type":"code","source":"if catboost_models and len(catboost_models) > 0:\n    print(\"\\nTop 20 Important Features (CatBoost):\")\n    feature_importance = catboost_models[0].get_feature_importance()\n    feature_names = X_train.columns\n    \n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': feature_importance\n    }).sort_values('Importance', ascending=False)\n    \n    print(importance_df.head(20).to_string(index=False))\n    \n    plt.figure(figsize=(10, 8))\n    top_features = importance_df.head(20)\n    plt.barh(range(len(top_features)), top_features['Importance'])\n    plt.yticks(range(len(top_features)), top_features['Feature'])\n    plt.xlabel('Importance')\n    plt.title('Top 20 Feature Importances', fontweight='bold')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.show()","metadata":{},"outputs":[],"execution_count":null}]}